#+TITLE: Apuntes de Recuperaci√≥n de Informaci√≥n Web
#+AUTHOR: Eduardo Alcaraz 
#+LANGUAGE: es
#+LaTeX_HEADER: \usepackage[spanish]{inputenc}
#+SETUPFILE: /home/likcos/Materias/Recuperacion/theme-readtheorg-local.setup
#+EXPORT_FILE_NAME: index.html
#+OPTIONS: num:nil
#+HTML_HEAD: <style> #content{max-width:1800px;}</style>
#+HTML_HEAD: <style>pre.src {background-color: #303030; color: #e5e5e5;}</style>


* Instalaci√≥n de requerimientos

** Verificaci√≥n de Python
Antes de instalar las dependencias, verifique que el sistema cuente con Python
versi√≥n 3.9 o superior.

#+BEGIN_SRC bash
python --version
#+END_SRC

En caso de que el comando anterior no est√© disponible, intente:

#+BEGIN_SRC bash
python3 --version
#+END_SRC

** Creaci√≥n del entorno virtual
Se recomienda crear un entorno virtual para aislar las dependencias del proyecto
y evitar conflictos entre versiones de librer√≠as.

#+BEGIN_SRC bash
python -m venv mlp_env
#+END_SRC

** Activaci√≥n del entorno virtual

La activaci√≥n del entorno virtual depende del sistema operativo y del int√©rprete
de comandos utilizado.

*** GNU/Linux y macOS
#+BEGIN_SRC bash
source mlp_env/bin/activate
#+END_SRC

*** Windows ‚Äì S√≠mbolo del sistema (CMD)
Si se utiliza el *S√≠mbolo del sistema* (cmd.exe), ejecute:

#+BEGIN_SRC bat
mlp_env\Scripts\activate.bat
#+END_SRC

*** Windows ‚Äì PowerShell
Si se utiliza *Windows PowerShell*, ejecute:

#+BEGIN_SRC powershell
mlp_env\Scripts\Activate.ps1
#+END_SRC

En caso de que la ejecuci√≥n de scripts est√© deshabilitada, habil√≠tela
temporalmente con:

#+BEGIN_SRC powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
#+END_SRC

Posteriormente, vuelva a ejecutar el comando de activaci√≥n.

*** Windows ‚Äì Git Bash
Si se utiliza *Git Bash*, ejecute:

#+BEGIN_SRC bash
source mlp_env/Scripts/activate
#+END_SRC

Una vez activado el entorno virtual, el nombre del entorno aparecer√° entre
par√©ntesis al inicio de la l√≠nea de comandos.

** Actualizaci√≥n del gestor de paquetes
Antes de instalar las librer√≠as, se recomienda actualizar el gestor de paquetes
=pip=.

#+BEGIN_SRC bash
pip install --upgrade pip
#+END_SRC

** Instalaci√≥n de dependencias
Ejecute el siguiente comando para instalar los requerimientos del m√≥dulo
Perceptr√≥n Multicapa *feedforward*.

#+BEGIN_SRC bash
pip install numpy matplotlib scikit-learn feedparser
#+END_SRC

** Verificaci√≥n de la instalaci√≥n
Para verificar que las dependencias se instalaron correctamente, ejecute Python
en modo interactivo:

#+BEGIN_SRC bash
python
#+END_SRC

Posteriormente, importe las librer√≠as:

#+BEGIN_SRC python
import numpy
import matplotlib
import sklearn
import feedparser
print("Instalaci√≥n de requerimientos completada correctamente")
#+END_SRC

** Desactivaci√≥n del entorno virtual
Una vez finalizado el trabajo, el entorno virtual puede desactivarse con el
siguiente comando:

#+BEGIN_SRC bash
deactivate
#+END_SRC


* Manual de Entornos Virtuales en Python

** Introducci√≥n
El uso de entornos virtuales es esencial para mantener las
dependencias de tus proyectos aisladas. En este manual aprender√°s a
gestionarlos usando el m√≥dulo est√°ndar =venv=.

** Flujo de Trabajo B√°sico

*** 1. Creaci√≥n del entorno
Para crear un entorno virtual, navega a la ra√≠z de tu proyecto en la terminal (o dentro de un buffer de Emacs con =M-x shell=) y ejecuta:

#+begin_src bash
python -m venv .venv
#+end_src

*Nota:* El nombre =.venv= es una convenci√≥n que hace que el directorio sea oculto en sistemas Unix.

*** 2. Activaci√≥n
La activaci√≥n depende de tu sistema operativo:

**** En Windows (PowerShell)
#+begin_src powershell
.\.venv\Scripts\Activate.ps1
#+end_src

**** En macOS / Linux
#+begin_src bash
source .venv/bin/activate
#+end_src

** Gesti√≥n de paquetes
Una vez activado (ver√°s el prefijo =(.venv)= en tu prompt), puedes instalar librer√≠as:

#+begin_src bash
pip install requests pandas
#+end_src


** El archivo de Requerimientos
Es fundamental para la reproducibilidad del proyecto.

** Exportar dependencias
#+begin_src bash
pip freeze > requirements.txt
#+end_src

** Instalar desde el archivo
#+begin_src bash
pip install -r requirements.txt
#+end_src

** Tips de Limpieza
Para salir del entorno virtual:
#+begin_src bash
deactivate
#+end_src

Para borrar el entorno, simplemente elimina la carpeta:
#+begin_src bash
rm -rf .venv  # En Linux/macOS
rmdir /s /q .venv  # En Windows
#+end_src

---
#+BEGIN_QUOTE
"Keep your global Python clean, keep your projects isolated."
#+END_QUOTE




** Entornos Virtuales Python (Edici√≥n Windows)

** Requisitos Previos
1. Tener instalado Python (descargado de [[https://www.python.org/][python.org]] o la Microsoft Store).
2. Durante la instalaci√≥n, aseg√∫rate de marcar la casilla: **"Add Python to PATH"**.

** Flujo de Trabajo en Windows

*** 1. Crear el Entorno Virtual
Abre tu terminal (PowerShell o CMD) en la carpeta de tu proyecto. El comando es el mismo para ambos:

#+begin_src powershell
python -m venv venv
#+end_src

*** 2. El Paso Cr√≠tico: La Activaci√≥n
En Windows, la activaci√≥n depende de qu√© terminal est√©s usando.

**** Opci√≥n A: PowerShell (Recomendado)
Si es la primera vez que usas scripts en Windows, podr√≠as recibir un error de seguridad. Primero, ejecuta esto como administrador (solo una vez):
#+begin_src powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
#+end_src

Luego, para activar el entorno:
#+begin_src powershell
.\venv\Scripts\Activate.ps1
#+end_src

**** Opci√≥n B: S√≠mbolo del Sistema (CMD)
#+begin_src cmd
.\venv\Scripts\activate.bat
#+end_src

*** 3. Confirmaci√≥n
Sabr√°s que el entorno est√° activo porque el nombre =(venv)= aparecer√° a la izquierda de la ruta en tu terminal:
#+example
(venv) C:\Proyectos\MiProyecto>
#+example

** Gesti√≥n de Librer√≠as con PIP

*** Instalaci√≥n de paquetes
Una vez activo el entorno, instala lo que necesites:
#+begin_src powershell
pip install pandas requests openpyxl
#+end_src

*** Congelar dependencias (Compartir proyecto)
Para que otros participantes tengan exactamente lo mismo que t√∫:
#+begin_src powershell
pip freeze > requirements.txt
#+end_src

*** Instalar desde un archivo recibido
Si un compa√±ero te pasa su =requirements.txt=:
#+begin_src powershell
pip install -r requirements.txt
#+end_src

** Uso de Entornos en Emacs (Windows)

Para que Emacs en Windows gestione bien el entorno, a√±ade esto a tu archivo de configuraci√≥n (=init.el= o =.emacs=):

*** Instalaci√≥n del paquete pyvenv
#+begin_src elisp
(use-package pyvenv
  :ensure t
  :config
  (pyvenv-mode 1))
#+end_src

*** C√≥mo activarlo dentro de Emacs
1. Presiona =M-x pyvenv-activate=.
2. Emacs te pedir√° la ruta. Navega hasta la carpeta =venv= de tu proyecto.
3. Al seleccionarla, Emacs usar√° ese int√©rprete de Python para todos los scripts que ejecutes.

** Soluci√≥n de Problemas Comunes en Windows

| Error / Problema | Soluci√≥n |
| :--- | :--- |
| "python" no se reconoce | Reinstala Python y marca "Add to PATH" o usa el comando `py`. |
| Error de "Execution Policy" | Ejecuta `Set-ExecutionPolicy RemoteSigned -Scope CurrentUser`. |
| No aparece el (venv) | Aseg√∫rate de usar el comando de activaci√≥n correcto para tu terminal (.ps1 vs .bat). |

** Desactivaci√≥n y Limpieza
Para salir del entorno:
#+begin_src powershell
deactivate
#+end_src

Si quieres borrar el entorno por completo (para empezar de cero):
#+begin_src powershell
rmdir /s /q venv
#+end_src

---
#+BEGIN_IMPORTANT
*Recordatorio:* Nunca incluyas la carpeta =venv= en tus archivos compartidos o en tu repositorio de Git. Solo comparte el c√≥digo y el archivo =requirements.txt=.
#+END_IMPORTANT








** Agregar a jupyter notebook

#+BEGIN_SRC shell
pip install ipykernel
python -m ipykernel install --user --name=redes --display-name="redes"
#+END_SRC






* Sistemas de recuperaci√≥n de informaci√≥n

** Introducci√≥n a los sistemas de recuperaci√≥n de informaci√≥n (IRS)

*** Definici√≥n y alcance

Un *Sistema de Recuperaci√≥n de Informaci√≥n* (IRS, por sus siglas en ingl√©s
*Information Retrieval System*) es un sistema software dise√±ado para almacenar,
representar y recuperar informaci√≥n relevante ante consultas de usuarios.

**** Caracter√≠sticas principales
- Almacena grandes vol√∫menes de documentos (texto, multimedia, metadatos).
- Permite consultas en lenguaje natural o estructurado.
- Devuelve un *ranking* o lista ordenada de documentos seg√∫n relevancia.
- La relevancia suele ser subjetiva y dependiente del contexto del usuario.

**** Ejemplo de necesidad de informaci√≥n
#+BEGIN_EXAMPLE
Usuario: "Necesito saber c√≥mo se calcula la precisi√≥n en un sistema de b√∫squeda
para poder comparar dos motores que estamos evaluando en mi empresa."

El IRS no busca la frase exacta "c√≥mo se calcula la precisi√≥n"; busca documentos
que traten sobre *evaluaci√≥n*, *precisi√≥n*, *sistemas de b√∫squeda* y los ordena
seg√∫n qu√© tan bien satisfacen esa necesidad.
#+END_EXAMPLE

*** Objetivo fundamental

El objetivo no es devolver *todos* los documentos que coincidan literalmente con
la consulta, sino aquellos que el usuario considerar√≠a *√∫tiles* o *relevantes*
para satisfacer su necesidad de informaci√≥n.

**** Diferencia con bases de datos

| Bases de datos          | Recuperaci√≥n de informaci√≥n        |
|-------------------------+------------------------------------|
| Consultas exactas (SQL) | Consultas por similitud/relevancia |
| Coincidencia exacta     | Ranking y ordenaci√≥n               |
| Datos estructurados     | Texto libre, documentos            |
| Respuesta determinista  | Respuesta aproximada, probabilista |

*** Componentes t√≠picos de un IRS

1. *√çndice* :: Estructura que permite localizar documentos sin escanear todo el corpus.
2. *Motor de b√∫squeda* :: Algoritmos que comparan consulta vs. documentos.
3. *Modelo de ranking* :: Criterio para ordenar resultados (TF-IDF, BM25, etc.).
4. *Interfaz de usuario* :: Formulario de b√∫squeda, resultados, filtros.
5. *M√≥dulo de evaluaci√≥n* :: M√©tricas para medir calidad (precisi√≥n, recall, etc.).

*** Aplicaciones

- Motores de b√∫squeda web (Google, Bing).
- B√∫squeda en bibliotecas digitales y repositorios.
- B√∫squeda en correo electr√≥nico y documentos corporativos.
- Sistemas de recomendaci√≥n y b√∫squeda sem√°ntica.

*** Ejemplo de flujo completo en un IRS

**** Corpus m√≠nimo de ejemplo
Se tienen 3 documentos (para ilustrar; en la realidad son millones):

| ID | Documento |
|----+-----------|
| d1 | "La recuperaci√≥n de informaci√≥n usa √≠ndices para buscar documentos de forma r√°pida." |
| d2 | "Los modelos vectoriales calculan similitud entre consulta y documento." |
| d3 | "La evaluaci√≥n mide precisi√≥n y recall del sistema de b√∫squeda." |

**** Paso 1: Indexaci√≥n
- Se extraen t√©rminos (tras eliminar stopwords y aplicar stemming): =recuperaci√≥n=, =informaci√≥n=, =√≠ndices=, =documentos=, =modelos=, =vectoriales=, =similitud=, =consulta=, =evaluaci√≥n=, =precisi√≥n=, =recall=, =b√∫squeda=.
- Se construye un *√≠ndice invertido*: para cada t√©rmino, lista de documentos que lo contienen.
- Ejemplo: =precisi√≥n= \(\rightarrow\) [d3], =documentos= \(\rightarrow\) [d1], =recuperaci√≥n= \(\rightarrow\) [d1], etc.

**** Paso 2: Consulta
- Usuario escribe: "evaluaci√≥n precisi√≥n b√∫squeda".

**** Paso 3: Recuperaci√≥n y ranking
- El motor obtiene candidatos del √≠ndice (p. ej. d3 contiene los tres t√©rminos) y aplica el modelo (TF-IDF o BM25) para puntuar cada documento.
- Resultado ordenado: d3 (mayor puntuaci√≥n), y posiblemente d1, d2 si comparten t√©rminos.

**** Paso 4: Presentaci√≥n
- Se muestra al usuario: t√≠tulo/snippet de d3 primero, luego los dem√°s, para que pueda elegir el documento que le resulta relevante.

** Interfaces de usuario para b√∫squeda

*** Funciones de la interfaz

La interfaz de usuario en un IRS debe permitir:

1. *Formular la consulta* (caja de b√∫squeda, operadores, filtros).
2. *Ver resultados* ordenados y con informaci√≥n suficiente para decidir.
3. *Refinar o reformular* la consulta (b√∫squeda iterativa).
4. *Acceder al documento* completo cuando se considera relevante.

*** Tipos de interfaces

**** Interfaz de consulta por palabras clave
- El usuario escribe t√©rminos (palabras clave).
- Puede usar operadores: AND, OR, NOT, comillas para frases.
- Ejemplo: =informaci√≥n AND (recuperaci√≥n OR b√∫squeda)=.

***** Ejemplos de consultas booleanas

| Consulta                           | Interpretaci√≥n                                          |
|------------------------------------+---------------------------------------------------------|
| recuperaci√≥n informaci√≥n           | documentos que contengan ambos t√©rminos (AND impl√≠cito) |
| "recuperaci√≥n de informaci√≥n"      | documentos con la frase exacta                          |
| TF-IDF OR BM25                     | documentos que contengan al menos uno de los dos        |
| evaluaci√≥n NOT recall              | documentos sobre evaluaci√≥n pero sin la palabra recall  |
| (precisi√≥n OR recall) AND m√©tricas | m√©tricas y adem√°s precisi√≥n o recall                    |


**** Interfaz de lenguaje natural
- Consultas en forma de pregunta o frase.
- El sistema interpreta la intenci√≥n (a veces con NLP).
- Ejemplo: "¬øC√≥mo se eval√∫a un sistema de recuperaci√≥n?"

***** Ejemplo
#+BEGIN_EXAMPLE
Usuario escribe: "recetas de pastel de chocolate sin gluten"

El sistema puede extraer: t√©rminos clave (recetas, pastel, chocolate, gluten),
negaci√≥n (sin gluten) y devolver documentos que hablen de pastel de chocolate
y que mencionen "sin gluten" o recetas aptas para cel√≠acos, aunque no aparezca
la frase exacta.
#+END_EXAMPLE

**** Interfaz con filtros y facetas
- Filtros por fecha, autor, tipo de documento, idioma.
- Facetas: categor√≠as o atributos para restringir resultados.
- Muy com√∫n en tiendas online y bibliotecas digitales.

***** Ejemplo (biblioteca digital)
#+BEGIN_EXAMPLE
B√∫squeda: "recuperaci√≥n de informaci√≥n"
Facetas mostradas al lado:
  Tipo de documento: Libro (120), Art√≠culo (85), Tesis (12)
  A√±o: 2020-2024 (45), 2015-2019 (98), anterior (74)
  Idioma: Espa√±ol (150), Ingl√©s (67)

El usuario hace clic en "Art√≠culo" y "2020-2024": la lista de resultados se
restringe a art√≠culos de esos a√±os, sin cambiar la consulta textual.
#+END_EXAMPLE

**** Interfaz de b√∫squeda por ejemplo (Query by Example)
- El usuario proporciona un documento o fragmento como "ejemplo" de lo que busca.
- El sistema busca documentos similares (b√∫squeda por similitud).

***** Ejemplo
#+BEGIN_EXAMPLE
Usuario pega un p√°rrafo de un art√≠culo que le gust√≥:
"El modelo vectorial representa documentos como vectores en un espacio de t√©rminos.
La similitud coseno mide el √°ngulo entre el vector de la consulta y el del documento."

El sistema trata ese texto como una "consulta larga" o como documento de referencia,
calcula su vector (o embedding) y busca en el corpus los documentos m√°s similares,
devolviendo por ejemplo art√≠culos sobre modelos vectoriales, TF-IDF y similitud coseno.
#+END_EXAMPLE

*** Elementos de presentaci√≥n de resultados

**** Lista de resultados (SERP)
- T√≠tulo, URL, snippet o fragmento del documento.
- Destacados (bold) de los t√©rminos de la consulta.
- Paginaci√≥n o scroll infinito.

**** Snippets
- Fragmentos cortos del documento donde aparecen los t√©rminos.
- Ayudan al usuario a juzgar relevancia sin abrir el documento.

**** Agrupaci√≥n y clustering
- Resultados agrupados por sitio, fecha o tema.
- Reduce redundancia y facilita la exploraci√≥n.

**** Ejemplo de SERP (p√°gina de resultados)
#+BEGIN_EXAMPLE
Consulta: "evaluaci√≥n recuperaci√≥n informaci√≥n"

--- Resultado 1 ---
T√≠tulo: Evaluaci√≥n de sistemas de recuperaci√≥n de informaci√≥n - Wikipedia
URL: https://es.wikipedia.org/wiki/...
Snippet: ... La evaluaci√≥n de la recuperaci√≥n de informaci√≥n utiliza m√©tricas como
precisi√≥n, recall y F1. Se usan colecciones de prueba con juicios de relevancia ...

--- Resultado 2 ---
T√≠tulo: Precisi√≥n y exhaustividad - Recuperaci√≥n de informaci√≥n
URL: https://...
Snippet: ... Para evaluar un sistema de recuperaci√≥n se definen la precisi√≥n (P) y
la exhaustividad (recall R). La precisi√≥n mide cu√°ntos de los recuperados son
relevantes ...

--- Resultado 3 ---
...
#+END_EXAMPLE
Los t√©rminos "evaluaci√≥n", "recuperaci√≥n", "informaci√≥n" aparecer√≠an resaltados
en negrita en t√≠tulo y snippet para que el usuario juzgue la relevancia de un vistazo.

*** Usabilidad y experiencia de usuario

- *Tiempo de respuesta*: resultados en milisegundos.
- *Claridad*: que el usuario entienda por qu√© aparece cada resultado.
- *Opciones de refinamiento*: sugerencias, "personas tambi√©n buscaron", filtros.
- *Accesibilidad*: uso con teclado, lectores de pantalla, dise√±o responsive.

** Modelos de recuperaci√≥n de informaci√≥n
:PROPERTIES:
:UNNUMBERED: t
:END:

Un *modelo de recuperaci√≥n* define c√≥mo se representan documentos y consultas, y
c√≥mo se calcula la relevancia o similitud entre ellos.

*** Modelo booleano

**** Idea
- Documentos y consultas como conjuntos de t√©rminos.
- La consulta es una expresi√≥n booleana (AND, OR, NOT).
- Un documento es "relevante" si satisface la expresi√≥n (verdadero/falso).

**** Limitaciones
- No hay ranking: todos los documentos que coinciden son iguales.
- No captura importancia de t√©rminos (frecuencia, rareza).
- Demasiado r√≠gido para el usuario promedio.

**** Ejemplo num√©rico (modelo booleano)
Corpus de 4 documentos (t√©rminos tras stemming):

| Doc | T√©rminos presentes                          |
|-----+--------------------------------------------|
| d1  | recuperaci√≥n, informaci√≥n, √≠ndice, documento |
| d2  | modelo, vectorial, similitud, documento     |
| d3  | evaluaci√≥n, precisi√≥n, recall, b√∫squeda    |
| d4  | evaluaci√≥n, informaci√≥n, √≠ndice             |

Consultas y resultados:
- \(q_1\) = =recuperaci√≥n AND informaci√≥n= \(\Rightarrow\) {d1} (solo d1 tiene ambos).
- \(q_2\) = =evaluaci√≥n OR precisi√≥n= \(\Rightarrow\) {d3, d4} (d3 tiene ambos; d4 tiene evaluaci√≥n).
- \(q_3\) = =documento NOT vectorial= \(\Rightarrow\) {d1} (d1 y d2 tienen "documento"; d2 tiene "vectorial", se excluye; d1 no tiene "vectorial", se incluye).

Todos los documentos devueltos se consideran "iguales"; no hay orden de preferencia.

*** Modelo vectorial

**** Idea
- Documentos y consultas como vectores en un espacio donde cada dimensi√≥n es un t√©rmino.
- Cada componente del vector es un peso (p. ej. TF-IDF).
- Similitud = similitud coseno entre vector de consulta y vector del documento.

**** F√≥rmula de similitud coseno
\[
\text{sim}(q, d) = \frac{\vec{q} \cdot \vec{d}}{|\vec{q}| \, |\vec{d}|}
\]

**** Ventajas
- Permite ranking (ordenar por similitud).
- Simple y eficiente.
- TF-IDF captura importancia de t√©rminos.

**** Ejemplo: espacio de t√©rminos
Supongamos vocabulario = {recuperaci√≥n, informaci√≥n, evaluaci√≥n}. Cada documento
y la consulta se representan como vectores de 3 dimensiones (pesos TF-IDF):

- \(q\) = "recuperaci√≥n informaci√≥n" \(\rightarrow\) \(\vec{q} = (0.8, 0.7, 0)\)
- \(d_1\) = "recuperaci√≥n de informaci√≥n e informaci√≥n" \(\rightarrow\) \(\vec{d_1} = (0.5, 0.9, 0)\)
- \(d_2\) = "evaluaci√≥n de la recuperaci√≥n" \(\rightarrow\) \(\vec{d_2} = (0.6, 0, 0.8)\)

Similitud coseno: \(\text{sim}(q,d) = \frac{\vec{q}\cdot\vec{d}}{|\vec{q}||\vec{d}|}\).
- \(\vec{q}\cdot\vec{d_1} = 0.8\cdot 0.5 + 0.7\cdot 0.9 = 0.4 + 0.63 = 1.03\) (alto).
- \(\vec{q}\cdot\vec{d_2} = 0.8\cdot 0.6 + 0 + 0 = 0.48\) (menor).

Tras normalizar por las normas, \(d_1\) queda por encima de \(d_2\) en el ranking,
que es lo esperado porque \(d_1\) comparte ambos t√©rminos de la consulta.

*** TF-IDF (Term Frequency - Inverse Document Frequency)

**** Frecuencia de t√©rmino (TF)
- Cu√°ntas veces aparece el t√©rmino en el documento.
- Variantes: TF bruto, TF logar√≠tmico (suavizado).

**** Frecuencia inversa de documento (IDF)
- \(\text{IDF}(t) = \log \frac{N}{n_t}\), donde \(N\) = n√∫mero de documentos, \(n_t\) = documentos que contienen \(t\).
- T√©rminos raros (en pocos documentos) tienen mayor IDF.

**** Peso TF-IDF
\[
w_{t,d} = \text{TF}(t,d) \times \text{IDF}(t)
\]

**** Ejemplo num√©rico TF-IDF
Corpus: 3 documentos. T√©rmino "recuperaci√≥n":
- En d1 aparece 3 veces; en d2 aparece 1 vez; en d3 no aparece.
- \(N = 3\), \(n_{\text{recuperaci√≥n}} = 2\) (est√° en d1 y d2).
- \(\text{IDF}(\text{recuperaci√≥n}) = \log \frac{3}{2} \approx 0.41\).

Para d1: \(\text{TF} = 3\) (o \(\log(1+3)\) si se suaviza). Peso \(\approx 3 \times 0.41 \approx 1.23\).
Para d2: \(\text{TF} = 1\). Peso \(\approx 1 \times 0.41 \approx 0.41\).

El t√©rmino "recuperaci√≥n" aporta m√°s peso en d1 que en d2 porque es m√°s frecuente
en d1; y aporta m√°s que un t√©rmino que aparezca en los 3 documentos (IDF menor).

*** Modelo probabil√≠stico (BM25, etc.)

**** Idea
- Estimar la probabilidad de que un documento sea relevante dada la consulta.
- Ordenar por \(P(\text{relevante} \mid d, q)\).

**** BM25
- Extensi√≥n del modelo probabil√≠stico; muy usado en pr√°ctica.
- Incorpora longitud del documento (penalizaci√≥n por documentos muy largos).
- Par√°metros: \(k_1\), \(b\) para ajustar saturaci√≥n de TF y efecto de la longitud.

**** Ejemplo intuitivo BM25
Documento A: 50 palabras, "recuperaci√≥n" aparece 2 veces.
Documento B: 500 palabras, "recuperaci√≥n" aparece 10 veces.

En TF bruto, B tendr√≠a mayor TF. Pero BM25:
- Satura el TF: 10 apariciones no aportan 5 veces m√°s que 2 (crecimiento sublineal).
- Penaliza por longitud: un documento muy largo tiene m√°s probabilidad de contener
  el t√©rmino por casualidad; BM25 reduce el peso seg√∫n la longitud respecto a la
  media del corpus. As√≠, un documento corto y focalizado (A) puede rankear m√°s alto.

*** Modelos basados en lenguaje (Language Models)

**** Idea
- Modelar cada documento como una distribuci√≥n de probabilidad sobre t√©rminos (language model).
- La consulta se "genera" con cierta probabilidad desde el modelo del documento.
- Ranking por \(P(q \mid d)\) o variantes (e.g. mezcla con modelo de colecci√≥n).

**** Ventajas
- Base te√≥rica s√≥lida (probabilidad).
- Permite suavizado (smoothing) para t√©rminos no vistos.

*** Resumen comparativo

| Modelo        | Ranking | Complejidad | Uso t√≠pico           |
|---------------+---------+-------------+----------------------|
| Booleano      | No      | Baja        | Sistemas legados     |
| Vectorial     | S√≠      | Media       | General, TF-IDF       |
| Probabil√≠stico| S√≠      | Media       | BM25 en Elasticsearch |
| Language Model| S√≠      | Mayor       | Investigaci√≥n, NLP    |

** Evaluaci√≥n de la recuperaci√≥n
:PROPERTIES:
:UNNUMBERED: t
:END:

La evaluaci√≥n permite comparar sistemas o configuraciones y decidir mejoras.

*** Conjuntos de prueba (test collections)

**** Componentes
1. *Corpus*: conjunto de documentos.
2. *Consultas* (topics): necesidades de informaci√≥n representativas.
3. *Juicios de relevancia* (qrels): para cada par (consulta, documento), si el documento es relevante o no (idealmente por humanos).

**** Ejemplos de colecciones
- Cranfield, TREC, CLEF, NTCIR: est√°ndar en investigaci√≥n.
- En la industria: datos propios con juicios impl√≠citos (clics, tiempo en p√°gina).

**** Ejemplo de colecci√≥n m√≠nima
- *Corpus*: 5 documentos d1, d2, d3, d4, d5.
- *Consulta*: "evaluaci√≥n de la recuperaci√≥n".
- *Juicios de relevancia* (qrels): un evaluador humano indica qu√© documentos son relevantes:
  - d1: no relevante.
  - d2: relevante.
  - d3: relevante.
  - d4: no relevante.
  - d5: relevante.
  Total relevante = 3 (d2, d3, d5).

*** M√©tricas principales

**** Precisi√≥n (Precision)
\[
P = \frac{\text{documentos relevantes recuperados}}{\text{total de documentos recuperados}}
\]
- "De lo que devolv√≠, ¬øcu√°nto era relevante?"

**** Recall (Exhaustividad)
\[
R = \frac{\text{documentos relevantes recuperados}}{\text{total de documentos relevantes en el corpus}}
\]
- "De todo lo relevante, ¬øcu√°nto recuper√©?"

**** F1 (F-measure)
- Media arm√≥nica de precisi√≥n y recall:
\[
F_1 = 2 \frac{P \cdot R}{P + R}
\]

**** Precisi√≥n en k (P@k)
- Precisi√≥n considerando solo los primeros \(k\) resultados.
- √ötil cuando el usuario solo mira los primeros resultados (p. ej. P@10).

**** Ejemplo num√©rico: precisi√≥n, recall, F1, P@k
Con la colecci√≥n anterior: 3 documentos relevantes (d2, d3, d5). Supongamos que
el sistema devuelve, en orden: [d2, d1, d3, d4, d5].

- *Recuperados*: 5. *Relevantes recuperados*: d2, d3, d5 \(\Rightarrow\) 3.
- Precisi√≥n \(P = 3/5 = 0.6\) (de los 5 devueltos, 3 son relevantes).
- Recall \(R = 3/3 = 1.0\) (recuperamos todos los relevantes).
- \(F_1 = 2 \cdot \frac{0.6 \cdot 1}{0.6 + 1} = \frac{1.2}{1.6} \approx 0.75\).

P@k:
- P@1 = 1/1 = 1 (el primero es relevante).
- P@2 = 1/2 = 0.5 (de los dos primeros, uno es relevante).
- P@3 = 2/3 \(\approx\) 0.67.
- P@5 = 3/5 = 0.6.

**** Precisi√≥n promedia (AP) y MAP
- *Average Precision (AP)*: para una consulta, promedio de precisiones en cada punto de recall donde se recupera un documento relevante.
- *MAP (Mean Average Precision)*: media de AP sobre todas las consultas.
- Muy usada en competiciones y literatura.

**** Ejemplo: Average Precision (AP)
Mismo ranking: [d2, d1, d3, d4, d5]; relevantes = {d2, d3, d5}.

En las posiciones 1, 2, 3, 4, 5 vamos marcando si el documento es relevante (R) o no (N):
pos 1: d2 R \(\rightarrow\) precisi√≥n hasta aqu√≠ = 1/1 = 1.0
pos 2: d1 N
pos 3: d3 R \(\rightarrow\) precisi√≥n hasta aqu√≠ = 2/3 \(\approx\) 0.67
pos 4: d4 N
pos 5: d5 R \(\rightarrow\) precisi√≥n hasta aqu√≠ = 3/5 = 0.6

AP = promedio de las precisiones en cada "hit" relevante:
\[
\text{AP} = \frac{1 + 2/3 + 3/5}{3} = \frac{1 + 0.67 + 0.6}{3} \approx 0.76
\]
Un ranking perfecto [d2, d3, d5, ...] tendr√≠a AP = 1.0.

**** NDCG (Normalized Discounted Cumulative Gain)
- Tiene en cuenta la *posici√≥n* en el ranking: m√°s relevante arriba es mejor.
- El "gain" se descuenta seg√∫n la posici√≥n (discounted).
- NDCG normaliza por el DCG ideal (ranking perfecto).
- Adecuado cuando hay grados de relevancia (no solo relevante/no relevante).

***** Ejemplo NDCG (intuitivo)
Supongamos relevancia en escala 0‚Äì3: 0 = no relevante, 3 = muy relevante.
Ranking del sistema: pos1 rel=2, pos2 rel=0, pos3 rel=3, pos4 rel=1.

DCG suma el "gain" (relevancia) descontado por la posici√≥n: \(\frac{\text{rel}}{\log_2(\text{pos}+1)}\).
- Pos 1: \(2/\log_2 2 = 2\)
- Pos 2: \(0\)
- Pos 3: \(3/\log_2 4 = 3/2 = 1.5\)
- Pos 4: \(1/\log_2 5 \approx 0.43\)
DCG \(\approx 2 + 0 + 1.5 + 0.43 \approx 3.93\).

El DCG ideal ordenar√≠a por relevancia descendente: [3, 2, 1, 0] \(\Rightarrow\) IDCG.
NDCG = DCG / IDCG (normalizado entre 0 y 1). As√≠ se premia que los m√°s relevantes
est√©n arriba en el ranking.

*** Evaluaci√≥n con usuarios

- *Estudios de usabilidad*: tiempo para completar tareas, satisfacci√≥n, n√∫mero de clics.
- *A/B testing*: comparar dos versiones del sistema con usuarios reales.
- *Juicios de relevancia*: coste humano; a veces se usan juicios impl√≠citos (clics, dwell time).

*** Trade-off precisi√≥n vs. recall

- Aumentar resultados mostrados \(\Rightarrow\) recall sube, precisi√≥n puede bajar.
- Ser m√°s estricto en el ranking \(\Rightarrow\) precisi√≥n sube, recall puede bajar.
- Depende del dominio: en legal/medicina a veces se prioriza recall; en web comercial, precisi√≥n en los primeros resultados.

**** Ejemplo
#+BEGIN_EXAMPLE
Corpus: 100 documentos, 10 relevantes para la consulta.

- Si el sistema devuelve solo los 5 primeros y los 5 son relevantes:
  P = 5/5 = 1.0, R = 5/10 = 0.5 (precisi√≥n perfecta, recall bajo).

- Si devuelve 50 documentos y 10 son relevantes:
  P = 10/50 = 0.2, R = 10/10 = 1.0 (recall perfecto, precisi√≥n baja).

- Objetivo t√≠pico: devolver unos 10‚Äì20 resultados con varios relevantes arriba,
  equilibrando P y R (p. ej. P@10 alto y recall razonable).
#+END_EXAMPLE

** Referencias 

- Manning, Raghavan, Sch√ºtze: /Information Retrieval: Implementing and Evaluating Search Engines/ (MIT Press).
- Baeza-Yates, Ribeiro-Neto: /Modern Information Retrieval/ (Addison Wesley).
- TREC: ~https://trec.nist.gov/~ (benchmarks y m√©tricas).


* Ejemplos IRS ‚Äî Recuperaci√≥n de informaci√≥n con feeds RSS 

Un *Sistema de Recuperaci√≥n de Informaci√≥n* (IRS, o IR en ingl√©s) es un sistema
que permite almacenar, organizar y recuperar documentos (o √≠tems) relevantes
ante una *consulta* del usuario. Los componentes t√≠picos son:

1. *Colecci√≥n*: conjunto de documentos (en nuestras actividades = √≠tems de feeds RSS).
2. *Consulta*: necesidad de informaci√≥n expresada por el usuario (query).
3. *Proceso de recuperaci√≥n*: matching entre consulta y documentos (ranking, filtrado).
4. *Interfaz*: forma en que el usuario formula consultas y ve resultados.

En estas actividades usaremos *feeds RSS* como colecci√≥n de documentos: cada
entrada (item) es un ‚Äúdocumento‚Äù con t√≠tulo, descripci√≥n, fecha y enlace.
As√≠ practicamos extracci√≥n, limpieza e indexaci√≥n como en un IRS real.

** Un buscador introductorio con feeds RSS

En este material construimos un *buscador muy sencillo* que usa *feeds RSS* como
fuente de documentos: t√∫ escribes una consulta y el sistema te devuelve √≠tems
relevantes (t√≠tulo, enlace, fecha). Para que el buscador tenga algo que buscar,
primero hay que *encontrar URLs de feeds RSS*. Por eso la primera actividad es
usar *hacks de b√∫squeda* en Google (o DuckDuckGo, etc.) para descubrir esas
p√°ginas; despu√©s extraemos, limpiamos e indexamos esos feeds y probamos la
b√∫squeda.

*** Hacks de b√∫squeda: encontrar p√°ginas y URLs de feeds RSS


El uso de operadores de b√∫squeda (tambi√©n conocidos como hacks de
b√∫squeda) permite refinar las consultas en buscadores web con el
objetivo de localizar p√°ginas que publican feeds RSS. Mediante estos
operadores es posible identificar de forma eficiente las URLs de
dichos feeds, los cuales constituyen la fuente primaria de documentos
para el sistema de recuperaci√≥n de informaci√≥n. Sin una colecci√≥n de
feeds RSS adecuadamente identificada, no es posible construir ni
alimentar el buscador de noticias propuesto.

****  Operadores √∫tiles Google Search
| Operador       | Ejemplo                 | Descripci√≥n                          |
|----------------+-------------------------+--------------------------------------|
| site:          | site:bbc.com rss        | Busca RSS dentro de un dominio       |
| inurl:         | inurl:rss noticias      | Busca URLs que contengan "rss"       |
| intitle:       | intitle:rss technology  | Busca p√°ginas con "rss" en el t√≠tulo |
| filetype:      | filetype:xml rss        | Localiza archivos XML (feeds)        |
| "frase exacta" | "RSS feed" site:news    | Coincidencia exacta                  |
| OR             | rss OR "xml feed"       | B√∫squeda alternativa                 |
| -              | rss -podcast            | Excluye t√©rminos                     |
| allinurl:      | allinurl:rss feeds news | Todas las palabras en la URL         |


**** Operadores √∫tiles DuckDuckGo

| Operador       | Ejemplo              | Descripci√≥n                  |
|----------------+----------------------+------------------------------|
| site:          | site:reuters.com rss | Limita b√∫squeda a un dominio |
| inurl:         | inurl:rss            | Busca "rss" en la URL        |
| intitle:       | intitle:"rss feed"   | Busca en el t√≠tulo           |
| filetype:      | filetype:xml rss     | Archivos XML                 |
| "frase exacta" | "news rss feed"      | Coincidencia exacta          |
| -              | rss -video           | Excluir t√©rminos             |


**** Bing Search

| Operador       | Ejemplo             | Descripci√≥n               |
|----------------+---------------------+---------------------------|
| site:          | site:elpais.com rss | Buscar RSS por dominio    |
| inurl:         | inurl:feed          | URLs que contienen "feed" |
| intitle:       | intitle:rss         | Buscar en t√≠tulos         |
| filetype:      | filetype:xml rss    | Feeds XML                 |
| "frase exacta" | "rss noticias"      | Coincidencia exacta       |
| -              | rss -audio          | Exclusi√≥n                 |

**** Hacks combinados (muy √∫tiles para IRS)
| Consulta ejemplo          |
|---------------------------|
| site:news "rss feed"      |
| inurl:rss filetype:xml    |
| site:.org intitle:rss     |
| site:gov filetype:xml rss |
| "rss feed" "news"         |



*** Consultas de ejemplo para probar en el buscador

#+begin_example
inurl:rss noticias tecnolog√≠a
inurl:feed blog educaci√≥n
"rss" "suscribirse" site:elpais.com
filetype:xml rss
inurl:atom feed
#+end_example

Desde el punto de vista del IRS, la *consulta* que se escribe en el buscador
(p. ej. ~inurl:rss noticias~) es la ‚Äúconsulta‚Äù del sistema de recuperaci√≥n; los
*resultados* que muestra Google (o DuckDuckGo, Bing) son la ‚Äúinterfaz de
b√∫squeda‚Äù: un listado de documentos (p√°ginas) que el modelo de recuperaci√≥n del
buscador consider√≥ relevantes. Las URLs de feeds que el usuario anota pasan a
ser la *colecci√≥n* que alimentar√° el peque√±o buscador sobre RSS.

** Extraer datos de un feed RSS y guardarlos
 
En un IRS la *colecci√≥n* es el conjunto de documentos sobre el que el sistema
responde a las consultas. Sin colecci√≥n no hay recuperaci√≥n. En nuestro caso,
los ‚Äúdocumentos‚Äù son los *√≠tems de un feed RSS*: cada entrada del feed (t√≠tulo,
enlace, fecha, resumen) equivale a un documento indexable. Extraer esos √≠tems
y representarlos en una estructura uniforme (p. ej. lista de diccionarios) es
el primer paso para que el IRS pueda comparar despu√©s la consulta del usuario
contra cada documento. Los feeds RSS son id√≥neos porque ya vienen estructurados
(XML con ~<item>~ o entradas Atom), con campos de texto (t√≠tulo, descripci√≥n)
que el IRS puede usar para b√∫squeda.

- *IRS ‚Äî Colecci√≥n*: la lista de √≠tems extra√≠dos del feed *es* la colecci√≥n del
  sistema.
- *RSS*: el feed es la fuente externa; cada ~entry~ o ~<item>~ es un documento.
- *C√≥digo*: descargar el feed (HTTP), parsear el XML y mapear cada √≠tem a
  campos (t√≠tulo, link, fecha, resumen) construye esa colecci√≥n en memoria.

La funci√≥n ~fetch_rss(url, max_entries)~ recibe la URL del feed y un l√≠mite de
entradas. ~feedparser.parse(url)~ descarga y parsea el feed (RSS o Atom) y
devuelve un objeto con ~.entries~: lista de √≠tems. Para cada entrada se
extraen ~title~, ~link~, ~published~ (o ~updated~) y ~summary~ (o ~description~).
Esos campos son los que un IRS necesita para identificar el documento y para
indexar texto (t√≠tulo y resumen). El resultado es una lista de diccionarios:
cada uno representa *un documento de la colecci√≥n*. Si hay error de red o XML
inv√°lido, se devuelve un √≠tem con ~error~ para no romper el flujo. Al final,
~items~ es la colecci√≥n sobre la que m√°s adelante se aplicar√° la consulta.

#+begin_src python :session irs :results output :exports both
import feedparser
import json

def fetch_rss(url, max_entries=20):
    """Extrae √≠tems de un feed RSS/Atom. Devuelve lista de diccionarios."""
    try:
        feed = feedparser.parse(url)
        items = []
        for e in feed.entries[:max_entries]:
            items.append({
                "title": e.get("title", ""),
                "link": e.get("link", ""),
                "published": e.get("published", e.get("updated", "")),
                "summary": e.get("summary", e.get("description", "")),
            })
        return items
    except Exception as err:
        return [{"error": str(err), "url": url}]

url = "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/section/mexico/portada"
items = fetch_rss(url, max_entries=15)
print(json.dumps(items, indent=2, ensure_ascii=False))
#+end_src

#+RESULTS:

Ejemplo con mas de una URL 

#+begin_src python :session irs :results output :exports both
import feedparser
import json

def fetch_rss(url, max_entries=20):
    """Extrae √≠tems de un feed RSS/Atom."""
    feed = feedparser.parse(url)
    items = []
    for e in feed.entries[:max_entries]:
        items.append({
            "title": e.get("title", ""),
            "link": e.get("link", ""),
            "published": e.get("published", e.get("updated", "")),
            "summary": e.get("summary", e.get("description", "")),
            "source": url
        })
    return items

# üîπ Lista de feeds
urls = [
    "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/section/mexico/portada",
    "https://www.reddit.com/r/python/.rss",
    "https://hnrss.org/frontpage"
]

all_items = []

for url in urls:
    try:
        all_items.extend(fetch_rss(url, max_entries=10))
    except Exception as err:
        all_items.append({"error": str(err), "url": url})

print(json.dumps(all_items, indent=2, ensure_ascii=False))
#+end_src


Ejemplo clasificando la salida

#+begin_src python :session irs :results output :exports both
import feedparser
import json

def fetch_rss(url, max_entries=20):
    """Devuelve una lista de √≠tems de un feed RSS/Atom."""
    feed = feedparser.parse(url)
    items = []
    for e in feed.entries[:max_entries]:
        items.append({
            "title": e.get("title", ""),
            "link": e.get("link", ""),
            "published": e.get("published", e.get("updated", "")),
            "summary": e.get("summary", e.get("description", "")),
        })
    return items

# üîπ Lista de feeds RSS / Atom
urls = [
    "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/section/mexico/portada",
    "https://www.reddit.com/r/python/.rss",
    "https://hnrss.org/frontpage"
]

# üîπ Diccionario: una clave por feed
feeds_data = {}

for url in urls:
    try:
        feeds_data[url] = fetch_rss(url, max_entries=10)
    except Exception as err:
        feeds_data[url] = [{"error": str(err)}]

# üîπ Salida legible en Org / consola
print(json.dumps(feeds_data, indent=2, ensure_ascii=False))
#+end_src



** Limpieza de texto (normalizaci√≥n para el IRS)
  
Los feeds RSS suelen incluir en t√≠tulo y resumen *HTML* (etiquetas ~<p>~, ~<a>~,
etc.), may√∫sculas/min√∫sculas mezcladas y signos de puntuaci√≥n. Si el IRS
busca la palabra ‚Äúpython‚Äù en el texto crudo, no encontrar√° ‚ÄúPython‚Äù ni
‚Äú<em>python</em>‚Äù. La *normalizaci√≥n* (limpieza) hace que todos los documentos
y la consulta compartan el mismo ‚Äúalfabeto‚Äù: min√∫sculas, sin HTML, sin
caracteres que no aporten para la b√∫squeda. As√≠ la comparaci√≥n consulta‚Äìdocumento
es consistente y el *modelo booleano* (palabras exactas) y el *vectorial* (conteo
de t√©rminos) pueden aplicarse sobre una representaci√≥n uniforme del texto.


- *IRS*: la normalizaci√≥n es una etapa t√≠pica de *indexaci√≥n*; el √≠ndice se
  construye sobre el texto limpio, no sobre el raw.
- *RSS*: los campos ~title~ y ~summary~/~description~ suelen venir en HTML;
  ~text_clean~ es la versi√≥n ‚Äúindexable‚Äù de ese contenido.
- *C√≥digo*: ~clean_text~ transforma una cadena en texto normalizado;
  ~normalize_items~ a√±ade a cada documento de la colecci√≥n el campo ~text_clean~
  (t√≠tulo + resumen concatenados y limpios).

clean_text(text)~: (1) quita etiquetas HTML con ~re.sub(r"<[^>]+>", " ", text)~;
(2) deja solo letras, n√∫meros y espacios con ~re.sub(r"[^\w\s]", " ", text,
flags=re.UNICODE)~; (3) colapsa espacios y pasa a min√∫sculas con
~.strip().lower()~. El resultado es una sola cadena en min√∫sculas, sin HTML ni
puntuaci√≥n, lista para comparar con la consulta. ~normalize_items(items)~
recorre cada √≠tem de la colecci√≥n, concatena t√≠tulo y resumen limpios en
~text_clean~ y modifica el √≠tem in-place. A partir de aqu√≠, la *recuperaci√≥n*
(booleana o rankeada) usar√° ~text_clean~, no el t√≠tulo/resumen originales.
As√≠ se garantiza que la b√∫squeda ‚Äúpython‚Äù coincida con ‚ÄúPython‚Äù en el feed.

#+begin_src python :session irs :results output :exports both
import re

def clean_text(text):
    """Limpieza b√°sica: quitar tags HTML, solo letras y espacios, min√∫sculas."""
    if not text:
        return ""
    text = re.sub(r"<[^>]+>", " ", text)
    text = re.sub(r"[^\w\s]", " ", text, flags=re.UNICODE)
    text = re.sub(r"\s+", " ", text).strip().lower()
    return text

def normalize_items(items):
    """A√±ade campo 'text_clean' a cada √≠tem (t√≠tulo + resumen limpios)."""
    for it in items:
        if "error" in it:
            continue
        title = it.get("title", "")
        summary = it.get("summary", "")
        it["text_clean"] = clean_text(title) + " " + clean_text(summary)
    return items

items_norm = normalize_items(items)
for i, it in enumerate(items_norm[:3]):
    if "error" not in it:
        print("--- √çtem", i+1, "---")
        print("text_clean:", it.get("text_clean", "")[:200], "...")
        print()
#+end_src

** Modelo booleano: b√∫squeda por palabras clave


En el *modelo booleano* de recuperaci√≥n, cada documento es *relevante* o *no
relevante* seg√∫n cumpla o no una expresi√≥n l√≥gica sobre los t√©rminos (AND, OR,
NOT). No hay ‚Äúm√°s o menos relevante‚Äù: es un filtro. Es el modelo m√°s directo
para consultas del tipo ‚Äúquiero √≠tems que contengan *todas* estas palabras‚Äù.
Sobre la colecci√≥n de √≠tems RSS normalizados (~text_clean~), una b√∫squeda AND
equivale a: ‚Äúdevolver solo los documentos cuyo ~text_clean~ contiene cada
palabra de la consulta‚Äù. As√≠ el IRS aplica el *proceso de recuperaci√≥n*:
comparar la *consulta* (query) con cada *documento* de la colecci√≥n y decidir
s√≠/no seg√∫n el criterio booleano.

- *IRS ‚Äî Proceso de recuperaci√≥n*: la funci√≥n que compara consulta y documentos
  implementa el *modelo booleano* (AND).
- *Consulta*: la cadena que escribe el usuario (p. ej. ‚Äúpython software‚Äù) se
  normaliza igual que los documentos y se divide en palabras; cada palabra debe
  aparecer en ~text_clean~.
- *RSS*: cada √≠tem del feed es un documento; el campo ~text_clean~ (t√≠tulo +
  resumen limpios) es el texto sobre el que se hace la comparaci√≥n.

search_boolean_and(items, query)~ recibe la colecci√≥n (lista de √≠tems con
~text_clean~) y la consulta en lenguaje natural. Primero se *normaliza la
consulta* con ~clean_text(query).split()~: mismas reglas que los documentos,
para que ‚ÄúPython Software‚Äù y ‚Äúpython software‚Äù den las mismas palabras. Luego
se recorre cada √≠tem: si ~text_clean~ contiene *todas* las palabras de la
consulta (~all(w in text for w in words)~), el documento se considera relevante
y se a√±ade a ~results~. La lista devuelta es el *conjunto recuperado*: documentos
que pasan el filtro booleano. No hay orden de relevancia; si se quisiera
ordenar, se usar√≠a un modelo con score (p. ej. el ranking siguiente).

#+begin_src python :session irs :results output :exports both
def search_boolean_and(items, query):
    """Recupera √≠tems donde text_clean contiene TODAS las palabras de query."""
    words = clean_text(query).split()
    results = []
    for it in items:
        if "error" in it:
            continue
        text = it.get("text_clean", "")
        if all(w in text for w in words):
            results.append(it)
    return results

query = "python software"
found = search_boolean_and(items_norm, query)
print("Consulta:", repr(query))
print("Resultados:", len(found))
for it in found[:5]:
    print("-", it.get("title", "")[:60])
#+end_src

** Ranking simple (modelo tipo vectorial)
 
El modelo booleano devuelve un conjunto sin orden: todos los documentos
recuperados son ‚Äúigual de relevantes‚Äù. En la pr√°ctica el usuario espera ver
*primero* los m√°s relevantes. El *modelo vectorial* (y variantes) asigna a
cada documento un *score* de relevancia y ordena por ese score. Una
simplificaci√≥n muy usada es contar cu√°ntas veces aparecen los t√©rminos de la
consulta en el documento (*term frequency*, TF): m√°s apariciones suelen indicar
mayor relevancia. As√≠ el IRS no solo filtra (s√≠/no) sino que *rankea* los
resultados; la interfaz puede mostrar los ~top_k~ primeros. En feeds RSS, los
√≠tems con m√°s menciones de las palabras buscadas en t√≠tulo y resumen aparecen
arriba.

- *IRS ‚Äî Modelo de recuperaci√≥n*: aqu√≠ se implementa un *ranking* inspirado en
  el modelo vectorial (score por conteo de t√©rminos en ~text_clean~).
- *Consulta*: se normaliza y se divide en palabras; cada palabra contribuye al
  score del documento.
- *RSS*: cada √≠tem tiene ~text_clean~; el score es la suma de las frecuencias
  de los t√©rminos de la consulta en ese texto. Los √≠tems se ordenan por score
  descendente.

search_ranked(items, query, top_k)~ recibe la colecci√≥n normalizada, la
consulta y cu√°ntos resultados devolver. Para cada √≠tem se calcula ~score~ como
~sum(text.split().count(w) for w in words)~: n√∫mero total de apariciones de
cualquier t√©rmino de la consulta en ~text_clean~. Solo se consideran documentos
con score > 0. Se ordena la lista ~(score, √≠tem)~ por score descendente
(~key=lambda x: -x[0]~) y se devuelven los primeros ~top_k~ √≠tems. As√≠ la
salida es una *lista ordenada por relevancia*: el primer elemento es el que m√°s
coincide con la consulta. Es una versi√≥n simplificada de TF (sin IDF ni
normalizaci√≥n por longitud); suficiente para ilustrar el concepto de ranking
en un IRS sobre feeds RSS.

#+begin_src python :session irs :results output :exports both
def search_ranked(items, query, top_k=10):
    """Recupera √≠tems rankeados por n√∫mero de apariciones de t√©rminos de la consulta."""
    words = clean_text(query).split()
    if not words:
        return []
    scored = []
    for it in items:
        if "error" in it:
            continue
        text = it.get("text_clean", "")
        score = sum(text.split().count(w) for w in words)
        if score > 0:
            scored.append((score, it))
    scored.sort(key=lambda x: -x[0])
    return [it for _, it in scored[:top_k]]

query2 = "python release"
ranked = search_ranked(items_norm, query2, top_k=5)
print("Consulta:", repr(query2))
for i, it in enumerate(ranked, 1):
    title = it.get("title", "")[:55]
    print(i, "-", title)
#+end_src

** Interfaz de resultados
  
En un IRS la *interfaz* es el medio por el que el usuario formula la consulta
y *ve los resultados*. Sin una presentaci√≥n clara de los documentos recuperados,
el sistema no es usable. En un buscador web t√≠pico la interfaz muestra t√≠tulo,
enlace, fecha y a veces un snippet. En nuestro flujo con feeds RSS y Org-mode,
la ‚Äúpantalla‚Äù de resultados puede ser una *tabla Org*: cada fila es un √≠tem
recuperado (por b√∫squeda booleana o rankeada), con columnas como n√∫mero de
orden, t√≠tulo, URL y fecha. As√≠ se cierra el ciclo IRS: colecci√≥n ‚Üí consulta ‚Üí
recuperaci√≥n ‚Üí *presentaci√≥n* al usuario.

- *IRS ‚Äî Interfaz*: la tabla generada es la *vista de resultados* del sistema;
  el usuario ve qu√© documentos se recuperaron y puede acceder al enlace (RSS
  ~link~) de cada √≠tem.
- *RSS*: los campos mostrados (t√≠tulo, enlace, fecha) vienen directamente del
  feed; el orden de las filas viene del *modelo de recuperaci√≥n* (ranking o
  filtro booleano).
- *C√≥digo*: ~results_to_org_table~ transforma la lista de √≠tems (salida de
  ~search_ranked~ o ~search_boolean_and~) en l√≠neas de tabla Org.

results_to_org_table(items_list)~ recibe la lista de √≠tems ya recuperados
(ordenados por score si vienen de ~search_ranked~). Construye la cabecera de
la tabla Org (~| # | T√≠tulo | Enlace | Fecha |~) y el separador (~|---+...~).
Para cada √≠tem a√±ade una fila: posici√≥n (~i~), t√≠tulo truncado a 50 caracteres,
~link~ (URL del √≠tem en el feed) y fecha truncada a 20 caracteres. El resultado
es una cadena con las l√≠neas de la tabla; en Org-mode se puede insertar en el
buffer o exportar. As√≠ el documento Org act√∫a como ‚Äúpantalla‚Äù del IRS: el usuario
ve el listado rankeado de √≠tems RSS con enlaces clicables.

#+begin_src python :session irs :results value :exports both
def results_to_org_table(items_list):
    """Genera l√≠neas de tabla Org (| col1 | col2 | col3 |)."""
    lines = ["| # | T√≠tulo | Enlace | Fecha |", "|---+--------+--------+-------|"]
    for i, it in enumerate(items_list, 1):
        if "error" in it:
            continue
        title = (it.get("title", "") or "")[:50]
        link = it.get("link", "")
        pub = (it.get("published", "") or "")[:20]
        lines.append(f"| {i} | {title} | {link} | {pub} |")
    return "\n".join(lines)

org_table = results_to_org_table(ranked)
print(org_table)
#+end_src

** Pipeline completo del IRS con RSS

Un IRS real encadena varias etapas: obtener la colecci√≥n, indexar/normalizar,
recibir la consulta, aplicar el modelo de recuperaci√≥n y presentar los
resultados. Integrar todo en un *pipeline* (una sola funci√≥n o flujo) permite
ver de un vistazo c√≥mo se relacionan los componentes del IRS con los feeds RSS:
URL del feed ‚Üí colecci√≥n; consulta del usuario ‚Üí proceso de recuperaci√≥n;
lista rankeada ‚Üí interfaz (tabla). As√≠ se evidencia que el IRS no es solo
‚Äúb√∫squeda‚Äù, sino *colecci√≥n + consulta + modelo + interfaz*.

*** Relaci√≥n IRS ‚Üî feeds RSS en el pipeline
| Componente IRS      | En el pipeline con RSS                          |
|---------------------+-------------------------------------------------|
| Colecci√≥n           | ~fetch_rss(feed_url)~ ‚Üí √≠tems del feed          |
| Normalizaci√≥n       | ~normalize_items(raw)~ ‚Üí ~text_clean~ por √≠tem |
| Consulta            | Par√°metro ~query~ (cadena del usuario)           |
| Modelo recuperaci√≥n | ~search_ranked(normalized, query, top_k)~       |
| Interfaz            | ~results_to_org_table(resultados)~ ‚Üí tabla Org  |

*** Explicaci√≥n del c√≥digo
~irs_pipeline(feed_url, query, max_entries, top_k)~ encadena: (1) ~fetch_rss~
descarga y parsea el feed y devuelve la lista de √≠tems (colecci√≥n en bruto);
(2) si hay error, se devuelve el mensaje; (3) ~normalize_items~ a√±ade
~text_clean~ a cada √≠tem; (4) ~search_ranked~ aplica el modelo de recuperaci√≥n
(ranking por conteo de t√©rminos) y devuelve los ~top_k~ √≠tems m√°s relevantes;
(5) opcionalmente se pasa esa lista a ~results_to_org_table~ para generar la
vista. Una sola llamada con URL de feed y consulta produce la tabla de
resultados que el usuario ver√≠a en la interfaz. As√≠ queda expl√≠cito el flujo
completo: feed RSS ‚Üí colecci√≥n ‚Üí normalizaci√≥n ‚Üí consulta ‚Üí ranking ‚Üí
presentaci√≥n.

#+begin_src python :session irs :results output :exports both
def irs_pipeline(feed_url, query, max_entries=20, top_k=5):
    """Pipeline: fetch ‚Üí normalizar ‚Üí b√∫squeda rankeada ‚Üí resultados."""
    raw = fetch_rss(feed_url, max_entries=max_entries)
    if raw and "error" in raw[0]:
        return raw
    normalized = normalize_items(raw)
    results = search_ranked(normalized, query, top_k=top_k)
    return results

url_ejemplo = "https://pyfound.blogspot.com/feeds/posts/default?alt=rss"
consulta = "python foundation"
resultados = irs_pipeline(url_ejemplo, consulta, max_entries=15, top_k=5)
print("Feed:", url_ejemplo)
print("Consulta:", consulta)
print(results_to_org_table(resultados))
#+end_src




