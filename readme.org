#+TITLE: Apuntes de Recuperaci√≥n de Informaci√≥n Web
#+AUTHOR: Eduardo Alcaraz 
#+LANGUAGE: es
#+LaTeX_HEADER: \usepackage[spanish]{inputenc}
#+SETUPFILE: /home/likcos/Materias/Recuperacion/theme-readtheorg-local.setup
#+EXPORT_FILE_NAME: index.html
#+OPTIONS: num:nil
#+HTML_HEAD: <style> #content{max-width:1800px;}</style>
#+HTML_HEAD: <style>pre.src {background-color: #303030; color: #e5e5e5;}</style>


* Instalaci√≥n de requerimientos

** Verificaci√≥n de Python
Antes de instalar las dependencias, verifique que el sistema cuente con Python
versi√≥n 3.9 o superior.

#+BEGIN_SRC bash
python --version
#+END_SRC

En caso de que el comando anterior no est√© disponible, intente:

#+BEGIN_SRC bash
python3 --version
#+END_SRC

** Creaci√≥n del entorno virtual
Se recomienda crear un entorno virtual para aislar las dependencias del proyecto
y evitar conflictos entre versiones de librer√≠as.

#+BEGIN_SRC bash
python -m venv mlp_env
#+END_SRC

** Activaci√≥n del entorno virtual

La activaci√≥n del entorno virtual depende del sistema operativo y del int√©rprete
de comandos utilizado.

*** GNU/Linux y macOS
#+BEGIN_SRC bash
source mlp_env/bin/activate
#+END_SRC

*** Windows ‚Äì S√≠mbolo del sistema (CMD)
Si se utiliza el *S√≠mbolo del sistema* (cmd.exe), ejecute:

#+BEGIN_SRC bat
mlp_env\Scripts\activate.bat
#+END_SRC

*** Windows ‚Äì PowerShell
Si se utiliza *Windows PowerShell*, ejecute:

#+BEGIN_SRC powershell
mlp_env\Scripts\Activate.ps1
#+END_SRC

En caso de que la ejecuci√≥n de scripts est√© deshabilitada, habil√≠tela
temporalmente con:

#+BEGIN_SRC powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
#+END_SRC

Posteriormente, vuelva a ejecutar el comando de activaci√≥n.

*** Windows ‚Äì Git Bash
Si se utiliza *Git Bash*, ejecute:

#+BEGIN_SRC bash
source mlp_env/Scripts/activate
#+END_SRC

Una vez activado el entorno virtual, el nombre del entorno aparecer√° entre
par√©ntesis al inicio de la l√≠nea de comandos.

** Actualizaci√≥n del gestor de paquetes
Antes de instalar las librer√≠as, se recomienda actualizar el gestor de paquetes
=pip=.

#+BEGIN_SRC bash
pip install --upgrade pip
#+END_SRC

** Instalaci√≥n de dependencias
Ejecute el siguiente comando para instalar los requerimientos del m√≥dulo
Perceptr√≥n Multicapa *feedforward*.

#+BEGIN_SRC bash
pip install numpy matplotlib scikit-learn feedparser
#+END_SRC

** Verificaci√≥n de la instalaci√≥n
Para verificar que las dependencias se instalaron correctamente, ejecute Python
en modo interactivo:

#+BEGIN_SRC bash
python
#+END_SRC

Posteriormente, importe las librer√≠as:

#+BEGIN_SRC python
import numpy
import matplotlib
import sklearn
import feedparser
print("Instalaci√≥n de requerimientos completada correctamente")
#+END_SRC

** Desactivaci√≥n del entorno virtual
Una vez finalizado el trabajo, el entorno virtual puede desactivarse con el
siguiente comando:

#+BEGIN_SRC bash
deactivate
#+END_SRC


* Manual de Entornos Virtuales en Python

** Introducci√≥n
El uso de entornos virtuales es esencial para mantener las
dependencias de tus proyectos aisladas. En este manual aprender√°s a
gestionarlos usando el m√≥dulo est√°ndar =venv=.

** Flujo de Trabajo B√°sico

*** 1. Creaci√≥n del entorno
Para crear un entorno virtual, navega a la ra√≠z de tu proyecto en la terminal (o dentro de un buffer de Emacs con =M-x shell=) y ejecuta:

#+begin_src bash
python -m venv .venv
#+end_src

*Nota:* El nombre =.venv= es una convenci√≥n que hace que el directorio sea oculto en sistemas Unix.

*** 2. Activaci√≥n
La activaci√≥n depende de tu sistema operativo:

**** En Windows (PowerShell)
#+begin_src powershell
.\.venv\Scripts\Activate.ps1
#+end_src

**** En macOS / Linux
#+begin_src bash
source .venv/bin/activate
#+end_src

** Gesti√≥n de paquetes
Una vez activado (ver√°s el prefijo =(.venv)= en tu prompt), puedes instalar librer√≠as:

#+begin_src bash
pip install requests pandas
#+end_src


** El archivo de Requerimientos
Es fundamental para la reproducibilidad del proyecto.

** Exportar dependencias
#+begin_src bash
pip freeze > requirements.txt
#+end_src

** Instalar desde el archivo
#+begin_src bash
pip install -r requirements.txt
#+end_src

** Tips de Limpieza
Para salir del entorno virtual:
#+begin_src bash
deactivate
#+end_src

Para borrar el entorno, simplemente elimina la carpeta:
#+begin_src bash
rm -rf .venv  # En Linux/macOS
rmdir /s /q .venv  # En Windows
#+end_src

---
#+BEGIN_QUOTE
"Keep your global Python clean, keep your projects isolated."
#+END_QUOTE




** Entornos Virtuales Python (Edici√≥n Windows)

** Requisitos Previos
1. Tener instalado Python (descargado de [[https://www.python.org/][python.org]] o la Microsoft Store).
2. Durante la instalaci√≥n, aseg√∫rate de marcar la casilla: **"Add Python to PATH"**.

** Flujo de Trabajo en Windows

*** 1. Crear el Entorno Virtual
Abre tu terminal (PowerShell o CMD) en la carpeta de tu proyecto. El comando es el mismo para ambos:

#+begin_src powershell
python -m venv venv
#+end_src

*** 2. El Paso Cr√≠tico: La Activaci√≥n
En Windows, la activaci√≥n depende de qu√© terminal est√©s usando.

**** Opci√≥n A: PowerShell (Recomendado)
Si es la primera vez que usas scripts en Windows, podr√≠as recibir un error de seguridad. Primero, ejecuta esto como administrador (solo una vez):
#+begin_src powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
#+end_src

Luego, para activar el entorno:
#+begin_src powershell
.\venv\Scripts\Activate.ps1
#+end_src

**** Opci√≥n B: S√≠mbolo del Sistema (CMD)
#+begin_src cmd
.\venv\Scripts\activate.bat
#+end_src

*** 3. Confirmaci√≥n
Sabr√°s que el entorno est√° activo porque el nombre =(venv)= aparecer√° a la izquierda de la ruta en tu terminal:
#+example
(venv) C:\Proyectos\MiProyecto>
#+example

** Gesti√≥n de Librer√≠as con PIP

*** Instalaci√≥n de paquetes
Una vez activo el entorno, instala lo que necesites:
#+begin_src powershell
pip install pandas requests openpyxl
#+end_src

*** Congelar dependencias (Compartir proyecto)
Para que otros participantes tengan exactamente lo mismo que t√∫:
#+begin_src powershell
pip freeze > requirements.txt
#+end_src

*** Instalar desde un archivo recibido
Si un compa√±ero te pasa su =requirements.txt=:
#+begin_src powershell
pip install -r requirements.txt
#+end_src

** Uso de Entornos en Emacs (Windows)

Para que Emacs en Windows gestione bien el entorno, a√±ade esto a tu archivo de configuraci√≥n (=init.el= o =.emacs=):

*** Instalaci√≥n del paquete pyvenv
#+begin_src elisp
(use-package pyvenv
  :ensure t
  :config
  (pyvenv-mode 1))
#+end_src

*** C√≥mo activarlo dentro de Emacs
1. Presiona =M-x pyvenv-activate=.
2. Emacs te pedir√° la ruta. Navega hasta la carpeta =venv= de tu proyecto.
3. Al seleccionarla, Emacs usar√° ese int√©rprete de Python para todos los scripts que ejecutes.

** Soluci√≥n de Problemas Comunes en Windows

| Error / Problema | Soluci√≥n |
| :--- | :--- |
| "python" no se reconoce | Reinstala Python y marca "Add to PATH" o usa el comando `py`. |
| Error de "Execution Policy" | Ejecuta `Set-ExecutionPolicy RemoteSigned -Scope CurrentUser`. |
| No aparece el (venv) | Aseg√∫rate de usar el comando de activaci√≥n correcto para tu terminal (.ps1 vs .bat). |

** Desactivaci√≥n y Limpieza
Para salir del entorno:
#+begin_src powershell
deactivate
#+end_src

Si quieres borrar el entorno por completo (para empezar de cero):
#+begin_src powershell
rmdir /s /q venv
#+end_src

---
#+BEGIN_IMPORTANT
*Recordatorio:* Nunca incluyas la carpeta =venv= en tus archivos compartidos o en tu repositorio de Git. Solo comparte el c√≥digo y el archivo =requirements.txt=.
#+END_IMPORTANT








** Agregar a jupyter notebook

#+BEGIN_SRC shell
pip install ipykernel
python -m ipykernel install --user --name=redes --display-name="redes"
#+END_SRC






* Sistemas de recuperaci√≥n de informaci√≥n

** Introducci√≥n a los sistemas de recuperaci√≥n de informaci√≥n (IRS)

*** Definici√≥n y alcance

Un *Sistema de Recuperaci√≥n de Informaci√≥n* (IRS, por sus siglas en ingl√©s
*Information Retrieval System*) es un sistema software dise√±ado para almacenar,
representar y recuperar informaci√≥n relevante ante consultas de usuarios.

**** Caracter√≠sticas principales
- Almacena grandes vol√∫menes de documentos (texto, multimedia, metadatos).
- Permite consultas en lenguaje natural o estructurado.
- Devuelve un *ranking* o lista ordenada de documentos seg√∫n relevancia.
- La relevancia suele ser subjetiva y dependiente del contexto del usuario.

**** Ejemplo de necesidad de informaci√≥n
#+BEGIN_EXAMPLE
Usuario: "Necesito saber c√≥mo se calcula la precisi√≥n en un sistema de b√∫squeda
para poder comparar dos motores que estamos evaluando en mi empresa."

El IRS no busca la frase exacta "c√≥mo se calcula la precisi√≥n"; busca documentos
que traten sobre *evaluaci√≥n*, *precisi√≥n*, *sistemas de b√∫squeda* y los ordena
seg√∫n qu√© tan bien satisfacen esa necesidad.
#+END_EXAMPLE

*** Objetivo fundamental

El objetivo no es devolver *todos* los documentos que coincidan literalmente con
la consulta, sino aquellos que el usuario considerar√≠a *√∫tiles* o *relevantes*
para satisfacer su necesidad de informaci√≥n.

**** Diferencia con bases de datos

| Bases de datos          | Recuperaci√≥n de informaci√≥n        |
|-------------------------+------------------------------------|
| Consultas exactas (SQL) | Consultas por similitud/relevancia |
| Coincidencia exacta     | Ranking y ordenaci√≥n               |
| Datos estructurados     | Texto libre, documentos            |
| Respuesta determinista  | Respuesta aproximada, probabilista |

*** Componentes t√≠picos de un IRS

1. *√çndice* :: Estructura que permite localizar documentos sin escanear todo el corpus.
2. *Motor de b√∫squeda* :: Algoritmos que comparan consulta vs. documentos.
3. *Modelo de ranking* :: Criterio para ordenar resultados (TF-IDF, BM25, etc.).
4. *Interfaz de usuario* :: Formulario de b√∫squeda, resultados, filtros.
5. *M√≥dulo de evaluaci√≥n* :: M√©tricas para medir calidad (precisi√≥n, recall, etc.).

*** Aplicaciones

- Motores de b√∫squeda web (Google, Bing).
- B√∫squeda en bibliotecas digitales y repositorios.
- B√∫squeda en correo electr√≥nico y documentos corporativos.
- Sistemas de recomendaci√≥n y b√∫squeda sem√°ntica.

*** Ejemplo de flujo completo en un IRS

**** Corpus m√≠nimo de ejemplo
Se tienen 3 documentos (para ilustrar; en la realidad son millones):

| ID | Documento |
|----+-----------|
| d1 | "La recuperaci√≥n de informaci√≥n usa √≠ndices para buscar documentos de forma r√°pida." |
| d2 | "Los modelos vectoriales calculan similitud entre consulta y documento." |
| d3 | "La evaluaci√≥n mide precisi√≥n y recall del sistema de b√∫squeda." |

**** Paso 1: Indexaci√≥n
- Se extraen t√©rminos (tras eliminar stopwords y aplicar stemming): =recuperaci√≥n=, =informaci√≥n=, =√≠ndices=, =documentos=, =modelos=, =vectoriales=, =similitud=, =consulta=, =evaluaci√≥n=, =precisi√≥n=, =recall=, =b√∫squeda=.
- Se construye un *√≠ndice invertido*: para cada t√©rmino, lista de documentos que lo contienen.
- Ejemplo: =precisi√≥n= \(\rightarrow\) [d3], =documentos= \(\rightarrow\) [d1], =recuperaci√≥n= \(\rightarrow\) [d1], etc.

**** Paso 2: Consulta
- Usuario escribe: "evaluaci√≥n precisi√≥n b√∫squeda".

**** Paso 3: Recuperaci√≥n y ranking
- El motor obtiene candidatos del √≠ndice (p. ej. d3 contiene los tres t√©rminos) y aplica el modelo (TF-IDF o BM25) para puntuar cada documento.
- Resultado ordenado: d3 (mayor puntuaci√≥n), y posiblemente d1, d2 si comparten t√©rminos.

**** Paso 4: Presentaci√≥n
- Se muestra al usuario: t√≠tulo/snippet de d3 primero, luego los dem√°s, para que pueda elegir el documento que le resulta relevante.

** Interfaces de usuario para b√∫squeda

*** Funciones de la interfaz

La interfaz de usuario en un IRS debe permitir:

1. *Formular la consulta* (caja de b√∫squeda, operadores, filtros).
2. *Ver resultados* ordenados y con informaci√≥n suficiente para decidir.
3. *Refinar o reformular* la consulta (b√∫squeda iterativa).
4. *Acceder al documento* completo cuando se considera relevante.

*** Tipos de interfaces

**** Interfaz de consulta por palabras clave
- El usuario escribe t√©rminos (palabras clave).
- Puede usar operadores: AND, OR, NOT, comillas para frases.
- Ejemplo: =informaci√≥n AND (recuperaci√≥n OR b√∫squeda)=.

***** Ejemplos de consultas booleanas

| Consulta                           | Interpretaci√≥n                                          |
|------------------------------------+---------------------------------------------------------|
| recuperaci√≥n informaci√≥n           | documentos que contengan ambos t√©rminos (AND impl√≠cito) |
| "recuperaci√≥n de informaci√≥n"      | documentos con la frase exacta                          |
| TF-IDF OR BM25                     | documentos que contengan al menos uno de los dos        |
| evaluaci√≥n NOT recall              | documentos sobre evaluaci√≥n pero sin la palabra recall  |
| (precisi√≥n OR recall) AND m√©tricas | m√©tricas y adem√°s precisi√≥n o recall                    |


**** Interfaz de lenguaje natural
- Consultas en forma de pregunta o frase.
- El sistema interpreta la intenci√≥n (a veces con NLP).
- Ejemplo: "¬øC√≥mo se eval√∫a un sistema de recuperaci√≥n?"

***** Ejemplo
#+BEGIN_EXAMPLE
Usuario escribe: "recetas de pastel de chocolate sin gluten"

El sistema puede extraer: t√©rminos clave (recetas, pastel, chocolate, gluten),
negaci√≥n (sin gluten) y devolver documentos que hablen de pastel de chocolate
y que mencionen "sin gluten" o recetas aptas para cel√≠acos, aunque no aparezca
la frase exacta.
#+END_EXAMPLE

**** Interfaz con filtros y facetas
- Filtros por fecha, autor, tipo de documento, idioma.
- Facetas: categor√≠as o atributos para restringir resultados.
- Muy com√∫n en tiendas online y bibliotecas digitales.

***** Ejemplo (biblioteca digital)
#+BEGIN_EXAMPLE
B√∫squeda: "recuperaci√≥n de informaci√≥n"
Facetas mostradas al lado:
  Tipo de documento: Libro (120), Art√≠culo (85), Tesis (12)
  A√±o: 2020-2024 (45), 2015-2019 (98), anterior (74)
  Idioma: Espa√±ol (150), Ingl√©s (67)

El usuario hace clic en "Art√≠culo" y "2020-2024": la lista de resultados se
restringe a art√≠culos de esos a√±os, sin cambiar la consulta textual.
#+END_EXAMPLE

**** Interfaz de b√∫squeda por ejemplo (Query by Example)
- El usuario proporciona un documento o fragmento como "ejemplo" de lo que busca.
- El sistema busca documentos similares (b√∫squeda por similitud).

***** Ejemplo
#+BEGIN_EXAMPLE
Usuario pega un p√°rrafo de un art√≠culo que le gust√≥:
"El modelo vectorial representa documentos como vectores en un espacio de t√©rminos.
La similitud coseno mide el √°ngulo entre el vector de la consulta y el del documento."

El sistema trata ese texto como una "consulta larga" o como documento de referencia,
calcula su vector (o embedding) y busca en el corpus los documentos m√°s similares,
devolviendo por ejemplo art√≠culos sobre modelos vectoriales, TF-IDF y similitud coseno.
#+END_EXAMPLE

*** Elementos de presentaci√≥n de resultados

**** Lista de resultados (SERP)
- T√≠tulo, URL, snippet o fragmento del documento.
- Destacados (bold) de los t√©rminos de la consulta.
- Paginaci√≥n o scroll infinito.

**** Snippets
- Fragmentos cortos del documento donde aparecen los t√©rminos.
- Ayudan al usuario a juzgar relevancia sin abrir el documento.

**** Agrupaci√≥n y clustering
- Resultados agrupados por sitio, fecha o tema.
- Reduce redundancia y facilita la exploraci√≥n.

**** Ejemplo de SERP (p√°gina de resultados)
#+BEGIN_EXAMPLE
Consulta: "evaluaci√≥n recuperaci√≥n informaci√≥n"

--- Resultado 1 ---
T√≠tulo: Evaluaci√≥n de sistemas de recuperaci√≥n de informaci√≥n - Wikipedia
URL: https://es.wikipedia.org/wiki/...
Snippet: ... La evaluaci√≥n de la recuperaci√≥n de informaci√≥n utiliza m√©tricas como
precisi√≥n, recall y F1. Se usan colecciones de prueba con juicios de relevancia ...

--- Resultado 2 ---
T√≠tulo: Precisi√≥n y exhaustividad - Recuperaci√≥n de informaci√≥n
URL: https://...
Snippet: ... Para evaluar un sistema de recuperaci√≥n se definen la precisi√≥n (P) y
la exhaustividad (recall R). La precisi√≥n mide cu√°ntos de los recuperados son
relevantes ...

--- Resultado 3 ---
...
#+END_EXAMPLE
Los t√©rminos "evaluaci√≥n", "recuperaci√≥n", "informaci√≥n" aparecer√≠an resaltados
en negrita en t√≠tulo y snippet para que el usuario juzgue la relevancia de un vistazo.

*** Usabilidad y experiencia de usuario

- *Tiempo de respuesta*: resultados en milisegundos.
- *Claridad*: que el usuario entienda por qu√© aparece cada resultado.
- *Opciones de refinamiento*: sugerencias, "personas tambi√©n buscaron", filtros.
- *Accesibilidad*: uso con teclado, lectores de pantalla, dise√±o responsive.

** Modelos de recuperaci√≥n de informaci√≥n
:PROPERTIES:
:UNNUMBERED: t
:END:

Un *modelo de recuperaci√≥n* define c√≥mo se representan documentos y consultas, y
c√≥mo se calcula la relevancia o similitud entre ellos.

*** Modelo booleano

**** Idea
- Documentos y consultas como conjuntos de t√©rminos.
- La consulta es una expresi√≥n booleana (AND, OR, NOT).
- Un documento es "relevante" si satisface la expresi√≥n (verdadero/falso).

**** Limitaciones
- No hay ranking: todos los documentos que coinciden son iguales.
- No captura importancia de t√©rminos (frecuencia, rareza).
- Demasiado r√≠gido para el usuario promedio.

**** Ejemplo num√©rico (modelo booleano)
Corpus de 4 documentos (t√©rminos tras stemming):

| Doc | T√©rminos presentes                          |
|-----+--------------------------------------------|
| d1  | recuperaci√≥n, informaci√≥n, √≠ndice, documento |
| d2  | modelo, vectorial, similitud, documento     |
| d3  | evaluaci√≥n, precisi√≥n, recall, b√∫squeda    |
| d4  | evaluaci√≥n, informaci√≥n, √≠ndice             |

Consultas y resultados:
- \(q_1\) = =recuperaci√≥n AND informaci√≥n= \(\Rightarrow\) {d1} (solo d1 tiene ambos).
- \(q_2\) = =evaluaci√≥n OR precisi√≥n= \(\Rightarrow\) {d3, d4} (d3 tiene ambos; d4 tiene evaluaci√≥n).
- \(q_3\) = =documento NOT vectorial= \(\Rightarrow\) {d1} (d1 y d2 tienen "documento"; d2 tiene "vectorial", se excluye; d1 no tiene "vectorial", se incluye).

Todos los documentos devueltos se consideran "iguales"; no hay orden de preferencia.

*** Modelo vectorial

**** Idea
- Documentos y consultas como vectores en un espacio donde cada dimensi√≥n es un t√©rmino.
- Cada componente del vector es un peso (p. ej. TF-IDF).
- Similitud = similitud coseno entre vector de consulta y vector del documento.

**** F√≥rmula de similitud coseno
\[
\text{sim}(q, d) = \frac{\vec{q} \cdot \vec{d}}{|\vec{q}| \, |\vec{d}|}
\]

**** Ventajas
- Permite ranking (ordenar por similitud).
- Simple y eficiente.
- TF-IDF captura importancia de t√©rminos.

**** Ejemplo: espacio de t√©rminos
Supongamos vocabulario = {recuperaci√≥n, informaci√≥n, evaluaci√≥n}. Cada documento
y la consulta se representan como vectores de 3 dimensiones (pesos TF-IDF):

- \(q\) = "recuperaci√≥n informaci√≥n" \(\rightarrow\) \(\vec{q} = (0.8, 0.7, 0)\)
- \(d_1\) = "recuperaci√≥n de informaci√≥n e informaci√≥n" \(\rightarrow\) \(\vec{d_1} = (0.5, 0.9, 0)\)
- \(d_2\) = "evaluaci√≥n de la recuperaci√≥n" \(\rightarrow\) \(\vec{d_2} = (0.6, 0, 0.8)\)

Similitud coseno: \(\text{sim}(q,d) = \frac{\vec{q}\cdot\vec{d}}{|\vec{q}||\vec{d}|}\).
- \(\vec{q}\cdot\vec{d_1} = 0.8\cdot 0.5 + 0.7\cdot 0.9 = 0.4 + 0.63 = 1.03\) (alto).
- \(\vec{q}\cdot\vec{d_2} = 0.8\cdot 0.6 + 0 + 0 = 0.48\) (menor).

Tras normalizar por las normas, \(d_1\) queda por encima de \(d_2\) en el ranking,
que es lo esperado porque \(d_1\) comparte ambos t√©rminos de la consulta.

*** TF-IDF (Term Frequency - Inverse Document Frequency)

**** Frecuencia de t√©rmino (TF)
- Cu√°ntas veces aparece el t√©rmino en el documento.
- Variantes: TF bruto, TF logar√≠tmico (suavizado).

**** Frecuencia inversa de documento (IDF)
- \(\text{IDF}(t) = \log \frac{N}{n_t}\), donde \(N\) = n√∫mero de documentos, \(n_t\) = documentos que contienen \(t\).
- T√©rminos raros (en pocos documentos) tienen mayor IDF.

**** Peso TF-IDF
\[
w_{t,d} = \text{TF}(t,d) \times \text{IDF}(t)
\]

**** Ejemplo num√©rico TF-IDF
Corpus: 3 documentos. T√©rmino "recuperaci√≥n":
- En d1 aparece 3 veces; en d2 aparece 1 vez; en d3 no aparece.
- \(N = 3\), \(n_{\text{recuperaci√≥n}} = 2\) (est√° en d1 y d2).
- \(\text{IDF}(\text{recuperaci√≥n}) = \log \frac{3}{2} \approx 0.41\).

Para d1: \(\text{TF} = 3\) (o \(\log(1+3)\) si se suaviza). Peso \(\approx 3 \times 0.41 \approx 1.23\).
Para d2: \(\text{TF} = 1\). Peso \(\approx 1 \times 0.41 \approx 0.41\).

El t√©rmino "recuperaci√≥n" aporta m√°s peso en d1 que en d2 porque es m√°s frecuente
en d1; y aporta m√°s que un t√©rmino que aparezca en los 3 documentos (IDF menor).

*** Modelo probabil√≠stico (BM25, etc.)

**** Idea
- Estimar la probabilidad de que un documento sea relevante dada la consulta.
- Ordenar por \(P(\text{relevante} \mid d, q)\).

**** BM25
- Extensi√≥n del modelo probabil√≠stico; muy usado en pr√°ctica.
- Incorpora longitud del documento (penalizaci√≥n por documentos muy largos).
- Par√°metros: \(k_1\), \(b\) para ajustar saturaci√≥n de TF y efecto de la longitud.

**** Ejemplo intuitivo BM25
Documento A: 50 palabras, "recuperaci√≥n" aparece 2 veces.
Documento B: 500 palabras, "recuperaci√≥n" aparece 10 veces.

En TF bruto, B tendr√≠a mayor TF. Pero BM25:
- Satura el TF: 10 apariciones no aportan 5 veces m√°s que 2 (crecimiento sublineal).
- Penaliza por longitud: un documento muy largo tiene m√°s probabilidad de contener
  el t√©rmino por casualidad; BM25 reduce el peso seg√∫n la longitud respecto a la
  media del corpus. As√≠, un documento corto y focalizado (A) puede rankear m√°s alto.

*** Modelos basados en lenguaje (Language Models)

**** Idea
- Modelar cada documento como una distribuci√≥n de probabilidad sobre t√©rminos (language model).
- La consulta se "genera" con cierta probabilidad desde el modelo del documento.
- Ranking por \(P(q \mid d)\) o variantes (e.g. mezcla con modelo de colecci√≥n).

**** Ventajas
- Base te√≥rica s√≥lida (probabilidad).
- Permite suavizado (smoothing) para t√©rminos no vistos.

*** Resumen comparativo

| Modelo        | Ranking | Complejidad | Uso t√≠pico           |
|---------------+---------+-------------+----------------------|
| Booleano      | No      | Baja        | Sistemas legados     |
| Vectorial     | S√≠      | Media       | General, TF-IDF       |
| Probabil√≠stico| S√≠      | Media       | BM25 en Elasticsearch |
| Language Model| S√≠      | Mayor       | Investigaci√≥n, NLP    |

** Evaluaci√≥n de la recuperaci√≥n
:PROPERTIES:
:UNNUMBERED: t
:END:

La evaluaci√≥n permite comparar sistemas o configuraciones y decidir mejoras.

*** Conjuntos de prueba (test collections)

**** Componentes
1. *Corpus*: conjunto de documentos.
2. *Consultas* (topics): necesidades de informaci√≥n representativas.
3. *Juicios de relevancia* (qrels): para cada par (consulta, documento), si el documento es relevante o no (idealmente por humanos).

**** Ejemplos de colecciones
- Cranfield, TREC, CLEF, NTCIR: est√°ndar en investigaci√≥n.
- En la industria: datos propios con juicios impl√≠citos (clics, tiempo en p√°gina).

**** Ejemplo de colecci√≥n m√≠nima
- *Corpus*: 5 documentos d1, d2, d3, d4, d5.
- *Consulta*: "evaluaci√≥n de la recuperaci√≥n".
- *Juicios de relevancia* (qrels): un evaluador humano indica qu√© documentos son relevantes:
  - d1: no relevante.
  - d2: relevante.
  - d3: relevante.
  - d4: no relevante.
  - d5: relevante.
  Total relevante = 3 (d2, d3, d5).

*** M√©tricas principales

**** Precisi√≥n (Precision)
\[
P = \frac{\text{documentos relevantes recuperados}}{\text{total de documentos recuperados}}
\]
- "De lo que devolv√≠, ¬øcu√°nto era relevante?"

**** Recall (Exhaustividad)
\[
R = \frac{\text{documentos relevantes recuperados}}{\text{total de documentos relevantes en el corpus}}
\]
- "De todo lo relevante, ¬øcu√°nto recuper√©?"

**** F1 (F-measure)
- Media arm√≥nica de precisi√≥n y recall:
\[
F_1 = 2 \frac{P \cdot R}{P + R}
\]

**** Precisi√≥n en k (P@k)
- Precisi√≥n considerando solo los primeros \(k\) resultados.
- √ötil cuando el usuario solo mira los primeros resultados (p. ej. P@10).

**** Ejemplo num√©rico: precisi√≥n, recall, F1, P@k
Con la colecci√≥n anterior: 3 documentos relevantes (d2, d3, d5). Supongamos que
el sistema devuelve, en orden: [d2, d1, d3, d4, d5].

- *Recuperados*: 5. *Relevantes recuperados*: d2, d3, d5 \(\Rightarrow\) 3.
- Precisi√≥n \(P = 3/5 = 0.6\) (de los 5 devueltos, 3 son relevantes).
- Recall \(R = 3/3 = 1.0\) (recuperamos todos los relevantes).
- \(F_1 = 2 \cdot \frac{0.6 \cdot 1}{0.6 + 1} = \frac{1.2}{1.6} \approx 0.75\).

P@k:
- P@1 = 1/1 = 1 (el primero es relevante).
- P@2 = 1/2 = 0.5 (de los dos primeros, uno es relevante).
- P@3 = 2/3 \(\approx\) 0.67.
- P@5 = 3/5 = 0.6.

**** Precisi√≥n promedia (AP) y MAP
- *Average Precision (AP)*: para una consulta, promedio de precisiones en cada punto de recall donde se recupera un documento relevante.
- *MAP (Mean Average Precision)*: media de AP sobre todas las consultas.
- Muy usada en competiciones y literatura.

**** Ejemplo: Average Precision (AP)
Mismo ranking: [d2, d1, d3, d4, d5]; relevantes = {d2, d3, d5}.

En las posiciones 1, 2, 3, 4, 5 vamos marcando si el documento es relevante (R) o no (N):
pos 1: d2 R \(\rightarrow\) precisi√≥n hasta aqu√≠ = 1/1 = 1.0
pos 2: d1 N
pos 3: d3 R \(\rightarrow\) precisi√≥n hasta aqu√≠ = 2/3 \(\approx\) 0.67
pos 4: d4 N
pos 5: d5 R \(\rightarrow\) precisi√≥n hasta aqu√≠ = 3/5 = 0.6

AP = promedio de las precisiones en cada "hit" relevante:
\[
\text{AP} = \frac{1 + 2/3 + 3/5}{3} = \frac{1 + 0.67 + 0.6}{3} \approx 0.76
\]
Un ranking perfecto [d2, d3, d5, ...] tendr√≠a AP = 1.0.

**** NDCG (Normalized Discounted Cumulative Gain)
- Tiene en cuenta la *posici√≥n* en el ranking: m√°s relevante arriba es mejor.
- El "gain" se descuenta seg√∫n la posici√≥n (discounted).
- NDCG normaliza por el DCG ideal (ranking perfecto).
- Adecuado cuando hay grados de relevancia (no solo relevante/no relevante).

***** Ejemplo NDCG (intuitivo)
Supongamos relevancia en escala 0‚Äì3: 0 = no relevante, 3 = muy relevante.
Ranking del sistema: pos1 rel=2, pos2 rel=0, pos3 rel=3, pos4 rel=1.

DCG suma el "gain" (relevancia) descontado por la posici√≥n: \(\frac{\text{rel}}{\log_2(\text{pos}+1)}\).
- Pos 1: \(2/\log_2 2 = 2\)
- Pos 2: \(0\)
- Pos 3: \(3/\log_2 4 = 3/2 = 1.5\)
- Pos 4: \(1/\log_2 5 \approx 0.43\)
DCG \(\approx 2 + 0 + 1.5 + 0.43 \approx 3.93\).

El DCG ideal ordenar√≠a por relevancia descendente: [3, 2, 1, 0] \(\Rightarrow\) IDCG.
NDCG = DCG / IDCG (normalizado entre 0 y 1). As√≠ se premia que los m√°s relevantes
est√©n arriba en el ranking.

*** Evaluaci√≥n con usuarios

- *Estudios de usabilidad*: tiempo para completar tareas, satisfacci√≥n, n√∫mero de clics.
- *A/B testing*: comparar dos versiones del sistema con usuarios reales.
- *Juicios de relevancia*: coste humano; a veces se usan juicios impl√≠citos (clics, dwell time).

*** Trade-off precisi√≥n vs. recall

- Aumentar resultados mostrados \(\Rightarrow\) recall sube, precisi√≥n puede bajar.
- Ser m√°s estricto en el ranking \(\Rightarrow\) precisi√≥n sube, recall puede bajar.
- Depende del dominio: en legal/medicina a veces se prioriza recall; en web comercial, precisi√≥n en los primeros resultados.

**** Ejemplo
#+BEGIN_EXAMPLE
Corpus: 100 documentos, 10 relevantes para la consulta.

- Si el sistema devuelve solo los 5 primeros y los 5 son relevantes:
  P = 5/5 = 1.0, R = 5/10 = 0.5 (precisi√≥n perfecta, recall bajo).

- Si devuelve 50 documentos y 10 son relevantes:
  P = 10/50 = 0.2, R = 10/10 = 1.0 (recall perfecto, precisi√≥n baja).

- Objetivo t√≠pico: devolver unos 10‚Äì20 resultados con varios relevantes arriba,
  equilibrando P y R (p. ej. P@10 alto y recall razonable).
#+END_EXAMPLE

** Referencias 

- Manning, Raghavan, Sch√ºtze: /Information Retrieval: Implementing and Evaluating Search Engines/ (MIT Press).
- Baeza-Yates, Ribeiro-Neto: /Modern Information Retrieval/ (Addison Wesley).
- TREC: ~https://trec.nist.gov/~ (benchmarks y m√©tricas).


* Ejemplos IRS ‚Äî Recuperaci√≥n de informaci√≥n con feeds RSS 

Un *Sistema de Recuperaci√≥n de Informaci√≥n* (IRS, o IR en ingl√©s) es un sistema
que permite almacenar, organizar y recuperar documentos (o √≠tems) relevantes
ante una *consulta* del usuario. Los componentes t√≠picos son:

1. *Colecci√≥n*: conjunto de documentos (en nuestras actividades = √≠tems de feeds RSS).
2. *Consulta*: necesidad de informaci√≥n expresada por el usuario (query).
3. *Proceso de recuperaci√≥n*: matching entre consulta y documentos (ranking, filtrado).
4. *Interfaz*: forma en que el usuario formula consultas y ve resultados.

En estas actividades usaremos *feeds RSS* como colecci√≥n de documentos: cada
entrada (item) es un ‚Äúdocumento‚Äù con t√≠tulo, descripci√≥n, fecha y enlace.
As√≠ practicamos extracci√≥n, limpieza e indexaci√≥n como en un IRS real.

** Un buscador introductorio con feeds RSS

En este material construimos un *buscador muy sencillo* que usa *feeds RSS* como
fuente de documentos: t√∫ escribes una consulta y el sistema te devuelve √≠tems
relevantes (t√≠tulo, enlace, fecha). Para que el buscador tenga algo que buscar,
primero hay que *encontrar URLs de feeds RSS*. Por eso la primera actividad es
usar *hacks de b√∫squeda* en Google (o DuckDuckGo, etc.) para descubrir esas
p√°ginas; despu√©s extraemos, limpiamos e indexamos esos feeds y probamos la
b√∫squeda.

*** Hacks de b√∫squeda: encontrar p√°ginas y URLs de feeds RSS


El uso de operadores de b√∫squeda (tambi√©n conocidos como hacks de
b√∫squeda) permite refinar las consultas en buscadores web con el
objetivo de localizar p√°ginas que publican feeds RSS. Mediante estos
operadores es posible identificar de forma eficiente las URLs de
dichos feeds, los cuales constituyen la fuente primaria de documentos
para el sistema de recuperaci√≥n de informaci√≥n. Sin una colecci√≥n de
feeds RSS adecuadamente identificada, no es posible construir ni
alimentar el buscador de noticias propuesto.

****  Operadores √∫tiles Google Search
| Operador       | Ejemplo                 | Descripci√≥n                          |
|----------------+-------------------------+--------------------------------------|
| site:          | site:bbc.com rss        | Busca RSS dentro de un dominio       |
| inurl:         | inurl:rss noticias      | Busca URLs que contengan "rss"       |
| intitle:       | intitle:rss technology  | Busca p√°ginas con "rss" en el t√≠tulo |
| filetype:      | filetype:xml rss        | Localiza archivos XML (feeds)        |
| "frase exacta" | "RSS feed" site:news    | Coincidencia exacta                  |
| OR             | rss OR "xml feed"       | B√∫squeda alternativa                 |
| -              | rss -podcast            | Excluye t√©rminos                     |
| allinurl:      | allinurl:rss feeds news | Todas las palabras en la URL         |


**** Operadores √∫tiles DuckDuckGo

| Operador       | Ejemplo              | Descripci√≥n                  |
|----------------+----------------------+------------------------------|
| site:          | site:reuters.com rss | Limita b√∫squeda a un dominio |
| inurl:         | inurl:rss            | Busca "rss" en la URL        |
| intitle:       | intitle:"rss feed"   | Busca en el t√≠tulo           |
| filetype:      | filetype:xml rss     | Archivos XML                 |
| "frase exacta" | "news rss feed"      | Coincidencia exacta          |
| -              | rss -video           | Excluir t√©rminos             |


**** Bing Search

| Operador       | Ejemplo             | Descripci√≥n               |
|----------------+---------------------+---------------------------|
| site:          | site:elpais.com rss | Buscar RSS por dominio    |
| inurl:         | inurl:feed          | URLs que contienen "feed" |
| intitle:       | intitle:rss         | Buscar en t√≠tulos         |
| filetype:      | filetype:xml rss    | Feeds XML                 |
| "frase exacta" | "rss noticias"      | Coincidencia exacta       |
| -              | rss -audio          | Exclusi√≥n                 |

**** Hacks combinados (muy √∫tiles para IRS)
| Consulta ejemplo          |
|---------------------------|
| site:news "rss feed"      |
| inurl:rss filetype:xml    |
| site:.org intitle:rss     |
| site:gov filetype:xml rss |
| "rss feed" "news"         |



*** Consultas de ejemplo para probar en el buscador

#+begin_example
inurl:rss noticias tecnolog√≠a
inurl:feed blog educaci√≥n
"rss" "suscribirse" site:elpais.com
filetype:xml rss
inurl:atom feed
#+end_example

Desde el punto de vista del IRS, la *consulta* que se escribe en el buscador
(p. ej. ~inurl:rss noticias~) es la ‚Äúconsulta‚Äù del sistema de recuperaci√≥n; los
*resultados* que muestra Google (o DuckDuckGo, Bing) son la ‚Äúinterfaz de
b√∫squeda‚Äù: un listado de documentos (p√°ginas) que el modelo de recuperaci√≥n del
buscador consider√≥ relevantes. Las URLs de feeds que el usuario anota pasan a
ser la *colecci√≥n* que alimentar√° el peque√±o buscador sobre RSS.

** Extraer datos de un feed RSS y guardarlos
 
En un IRS la *colecci√≥n* es el conjunto de documentos sobre el que el sistema
responde a las consultas. Sin colecci√≥n no hay recuperaci√≥n. En nuestro caso,
los ‚Äúdocumentos‚Äù son los *√≠tems de un feed RSS*: cada entrada del feed (t√≠tulo,
enlace, fecha, resumen) equivale a un documento indexable. Extraer esos √≠tems
y representarlos en una estructura uniforme (p. ej. lista de diccionarios) es
el primer paso para que el IRS pueda comparar despu√©s la consulta del usuario
contra cada documento. Los feeds RSS son id√≥neos porque ya vienen estructurados
(XML con ~<item>~ o entradas Atom), con campos de texto (t√≠tulo, descripci√≥n)
que el IRS puede usar para b√∫squeda.

- *IRS ‚Äî Colecci√≥n*: la lista de √≠tems extra√≠dos del feed *es* la colecci√≥n del
  sistema.
- *RSS*: el feed es la fuente externa; cada ~entry~ o ~<item>~ es un documento.
- *C√≥digo*: descargar el feed (HTTP), parsear el XML y mapear cada √≠tem a
  campos (t√≠tulo, link, fecha, resumen) construye esa colecci√≥n en memoria.

La funci√≥n ~fetch_rss(url, max_entries)~ recibe la URL del feed y un l√≠mite de
entradas. ~feedparser.parse(url)~ descarga y parsea el feed (RSS o Atom) y
devuelve un objeto con ~.entries~: lista de √≠tems. Para cada entrada se
extraen ~title~, ~link~, ~published~ (o ~updated~) y ~summary~ (o ~description~).
Esos campos son los que un IRS necesita para identificar el documento y para
indexar texto (t√≠tulo y resumen). El resultado es una lista de diccionarios:
cada uno representa *un documento de la colecci√≥n*. Si hay error de red o XML
inv√°lido, se devuelve un √≠tem con ~error~ para no romper el flujo. Al final,
~items~ es la colecci√≥n sobre la que m√°s adelante se aplicar√° la consulta.

#+begin_src python :session irs :results output :exports both
import feedparser
import json

def fetch_rss(url, max_entries=20):
    """Extrae √≠tems de un feed RSS/Atom. Devuelve lista de diccionarios."""
    try:
        feed = feedparser.parse(url)
        items = []
        for e in feed.entries[:max_entries]:
            items.append({
                "title": e.get("title", ""),
                "link": e.get("link", ""),
                "published": e.get("published", e.get("updated", "")),
                "summary": e.get("summary", e.get("description", "")),
            })
        return items
    except Exception as err:
        return [{"error": str(err), "url": url}]

url = "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/section/mexico/portada"
items = fetch_rss(url, max_entries=15)
print(json.dumps(items, indent=2, ensure_ascii=False))
#+end_src

#+RESULTS:

Ejemplo con mas de una URL 

#+begin_src python :session irs :results output :exports both
import feedparser
import json

def fetch_rss(url, max_entries=20):
    """Extrae √≠tems de un feed RSS/Atom."""
    feed = feedparser.parse(url)
    items = []
    for e in feed.entries[:max_entries]:
        items.append({
            "title": e.get("title", ""),
            "link": e.get("link", ""),
            "published": e.get("published", e.get("updated", "")),
            "summary": e.get("summary", e.get("description", "")),
            "source": url
        })
    return items

# üîπ Lista de feeds
urls = [
    "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/section/mexico/portada",
    "https://www.reddit.com/r/python/.rss",
    "https://hnrss.org/frontpage"
]

all_items = []

for url in urls:
    try:
        all_items.extend(fetch_rss(url, max_entries=10))
    except Exception as err:
        all_items.append({"error": str(err), "url": url})

print(json.dumps(all_items, indent=2, ensure_ascii=False))
#+end_src


Ejemplo clasificando la salida

#+begin_src python :session irs :results output :exports both
import feedparser
import json

def fetch_rss(url, max_entries=20):
    """Devuelve una lista de √≠tems de un feed RSS/Atom."""
    feed = feedparser.parse(url)
    items = []
    for e in feed.entries[:max_entries]:
        items.append({
            "title": e.get("title", ""),
            "link": e.get("link", ""),
            "published": e.get("published", e.get("updated", "")),
            "summary": e.get("summary", e.get("description", "")),
        })
    return items

# üîπ Lista de feeds RSS / Atom
urls = [
    "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/section/mexico/portada",
    "https://www.reddit.com/r/python/.rss",
    "https://hnrss.org/frontpage"
]

#  Diccionario: una clave por feed
feeds_data = {}

for url in urls:
    try:
        feeds_data[url] = fetch_rss(url, max_entries=10)
    except Exception as err:
        feeds_data[url] = [{"error": str(err)}]

#  Salida legible en Org / consola
print(json.dumps(feeds_data, indent=2, ensure_ascii=False))
#+end_src


Guardar en json

#+begin_src python :session irs :results output :exports both
import feedparser
import json

def fetch_rss(url, max_entries=20):
    """Devuelve una lista de √≠tems de un feed RSS/Atom."""
    feed = feedparser.parse(url)
    items = []
    for e in feed.entries[:max_entries]:
        items.append({
            "title": e.get("title", ""),
            "link": e.get("link", ""),
            "published": e.get("published", e.get("updated", "")),
            "summary": e.get("summary", e.get("description", "")),
        })
    return items

# üîπ Lista de feeds RSS / Atom
urls = [
    "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/section/mexico/portada",
    "https://www.reddit.com/r/python/.rss",
    "https://hnrss.org/frontpage"
]

#  Diccionario: una clave por feed
feeds_data = {}

for url in urls:
    try:
        feeds_data[url] = fetch_rss(url, max_entries=10)
    except Exception as err:
        feeds_data[url] = [{"error": str(err)}]

        
with open("data.json", "w", encoding="utf-8") as f:
    json.dump(feeds_data, f, indent=2, ensure_ascii=False)
        
        
#  Salida legible en Org / consola
print("Salida del JSON")
#+end_src

#+RESULTS:
: Salida del JSON



** Limpieza de texto (normalizaci√≥n para el IRS)
  
Los feeds RSS suelen incluir en t√≠tulo y resumen *HTML* (etiquetas ~<p>~, ~<a>~,
etc.), may√∫sculas/min√∫sculas mezcladas y signos de puntuaci√≥n. Si el IRS
busca la palabra ‚Äúpython‚Äù en el texto crudo, no encontrar√° ‚ÄúPython‚Äù ni
‚Äú<em>python</em>‚Äù. La *normalizaci√≥n* (limpieza) hace que todos los documentos
y la consulta compartan el mismo ‚Äúalfabeto‚Äù: min√∫sculas, sin HTML, sin
caracteres que no aporten para la b√∫squeda. As√≠ la comparaci√≥n consulta‚Äìdocumento
es consistente y el *modelo booleano* (palabras exactas) y el *vectorial* (conteo
de t√©rminos) pueden aplicarse sobre una representaci√≥n uniforme del texto.


- *IRS*: la normalizaci√≥n es una etapa t√≠pica de *indexaci√≥n*; el √≠ndice se
  construye sobre el texto limpio, no sobre el raw.
- *RSS*: los campos ~title~ y ~summary~/~description~ suelen venir en HTML;
  ~text_clean~ es la versi√≥n ‚Äúindexable‚Äù de ese contenido.
- *C√≥digo*: ~clean_text~ transforma una cadena en texto normalizado;
  ~normalize_items~ a√±ade a cada documento de la colecci√≥n el campo ~text_clean~
  (t√≠tulo + resumen concatenados y limpios).

clean_text(text)~: (1) quita etiquetas HTML con ~re.sub(r"<[^>]+>", " ", text)~;
(2) deja solo letras, n√∫meros y espacios con ~re.sub(r"[^\w\s]", " ", text,
flags=re.UNICODE)~; (3) colapsa espacios y pasa a min√∫sculas con
~.strip().lower()~. El resultado es una sola cadena en min√∫sculas, sin HTML ni
puntuaci√≥n, lista para comparar con la consulta. ~normalize_items(items)~
recorre cada √≠tem de la colecci√≥n, concatena t√≠tulo y resumen limpios en
~text_clean~ y modifica el √≠tem in-place. A partir de aqu√≠, la *recuperaci√≥n*
(booleana o rankeada) usar√° ~text_clean~, no el t√≠tulo/resumen originales.
As√≠ se garantiza que la b√∫squeda ‚Äúpython‚Äù coincida con ‚ÄúPython‚Äù en el feed.

#+begin_src python :session irs :results output :exports both
import re

def clean_text(text):
    """Limpieza b√°sica: quitar tags HTML, solo letras y espacios, min√∫sculas."""
    if not text:
        return ""
    text = re.sub(r"<[^>]+>", " ", text)
    text = re.sub(r"[^\w\s]", " ", text, flags=re.UNICODE)
    text = re.sub(r"\s+", " ", text).strip().lower()
    return text

def normalize_items(items):
    """A√±ade campo 'text_clean' a cada √≠tem (t√≠tulo + resumen limpios)."""
    for it in items:
        if "error" in it:
            continue
        title = it.get("title", "")
        summary = it.get("summary", "")
        it["text_clean"] = clean_text(title) + " " + clean_text(summary)
    return items

items_norm = normalize_items(items)
for i, it in enumerate(items_norm[:3]):
    if "error" not in it:
        print("--- √çtem", i+1, "---")
        print("text_clean:", it.get("text_clean", "")[:200], "...")
        print()
#+end_src

** Modelo booleano: b√∫squeda por palabras clave


En el *modelo booleano* de recuperaci√≥n, cada documento es *relevante* o *no
relevante* seg√∫n cumpla o no una expresi√≥n l√≥gica sobre los t√©rminos (AND, OR,
NOT). No hay ‚Äúm√°s o menos relevante‚Äù: es un filtro. Es el modelo m√°s directo
para consultas del tipo ‚Äúquiero √≠tems que contengan *todas* estas palabras‚Äù.
Sobre la colecci√≥n de √≠tems RSS normalizados (~text_clean~), una b√∫squeda AND
equivale a: ‚Äúdevolver solo los documentos cuyo ~text_clean~ contiene cada
palabra de la consulta‚Äù. As√≠ el IRS aplica el *proceso de recuperaci√≥n*:
comparar la *consulta* (query) con cada *documento* de la colecci√≥n y decidir
s√≠/no seg√∫n el criterio booleano.

- *IRS ‚Äî Proceso de recuperaci√≥n*: la funci√≥n que compara consulta y documentos
  implementa el *modelo booleano* (AND).
- *Consulta*: la cadena que escribe el usuario (p. ej. ‚Äúpython software‚Äù) se
  normaliza igual que los documentos y se divide en palabras; cada palabra debe
  aparecer en ~text_clean~.
- *RSS*: cada √≠tem del feed es un documento; el campo ~text_clean~ (t√≠tulo +
  resumen limpios) es el texto sobre el que se hace la comparaci√≥n.

search_boolean_and(items, query)~ recibe la colecci√≥n (lista de √≠tems con
~text_clean~) y la consulta en lenguaje natural. Primero se *normaliza la
consulta* con ~clean_text(query).split()~: mismas reglas que los documentos,
para que ‚ÄúPython Software‚Äù y ‚Äúpython software‚Äù den las mismas palabras. Luego
se recorre cada √≠tem: si ~text_clean~ contiene *todas* las palabras de la
consulta (~all(w in text for w in words)~), el documento se considera relevante
y se a√±ade a ~results~. La lista devuelta es el *conjunto recuperado*: documentos
que pasan el filtro booleano. No hay orden de relevancia; si se quisiera
ordenar, se usar√≠a un modelo con score (p. ej. el ranking siguiente).

#+begin_src python :session irs :results output :exports both
def search_boolean_and(items, query):
    """Recupera √≠tems donde text_clean contiene TODAS las palabras de query."""
    words = clean_text(query).split()
    results = []
    for it in items:
        if "error" in it:
            continue
        text = it.get("text_clean", "")
        if all(w in text for w in words):
            results.append(it)
    return results

query = "python software"
found = search_boolean_and(items_norm, query)
print("Consulta:", repr(query))
print("Resultados:", len(found))
for it in found[:5]:
    print("-", it.get("title", "")[:60])
#+end_src

** Ranking simple (modelo tipo vectorial)
 
El modelo booleano devuelve un conjunto sin orden: todos los documentos
recuperados son ‚Äúigual de relevantes‚Äù. En la pr√°ctica el usuario espera ver
*primero* los m√°s relevantes. El *modelo vectorial* (y variantes) asigna a
cada documento un *score* de relevancia y ordena por ese score. Una
simplificaci√≥n muy usada es contar cu√°ntas veces aparecen los t√©rminos de la
consulta en el documento (*term frequency*, TF): m√°s apariciones suelen indicar
mayor relevancia. As√≠ el IRS no solo filtra (s√≠/no) sino que *rankea* los
resultados; la interfaz puede mostrar los ~top_k~ primeros. En feeds RSS, los
√≠tems con m√°s menciones de las palabras buscadas en t√≠tulo y resumen aparecen
arriba.

- *IRS ‚Äî Modelo de recuperaci√≥n*: aqu√≠ se implementa un *ranking* inspirado en
  el modelo vectorial (score por conteo de t√©rminos en ~text_clean~).
- *Consulta*: se normaliza y se divide en palabras; cada palabra contribuye al
  score del documento.
- *RSS*: cada √≠tem tiene ~text_clean~; el score es la suma de las frecuencias
  de los t√©rminos de la consulta en ese texto. Los √≠tems se ordenan por score
  descendente.

search_ranked(items, query, top_k)~ recibe la colecci√≥n normalizada, la
consulta y cu√°ntos resultados devolver. Para cada √≠tem se calcula ~score~ como
~sum(text.split().count(w) for w in words)~: n√∫mero total de apariciones de
cualquier t√©rmino de la consulta en ~text_clean~. Solo se consideran documentos
con score > 0. Se ordena la lista ~(score, √≠tem)~ por score descendente
(~key=lambda x: -x[0]~) y se devuelven los primeros ~top_k~ √≠tems. As√≠ la
salida es una *lista ordenada por relevancia*: el primer elemento es el que m√°s
coincide con la consulta. Es una versi√≥n simplificada de TF (sin IDF ni
normalizaci√≥n por longitud); suficiente para ilustrar el concepto de ranking
en un IRS sobre feeds RSS.

#+begin_src python :session irs :results output :exports both
def search_ranked(items, query, top_k=10):
    """Recupera √≠tems rankeados por n√∫mero de apariciones de t√©rminos de la consulta."""
    words = clean_text(query).split()
    if not words:
        return []
    scored = []
    for it in items:
        if "error" in it:
            continue
        text = it.get("text_clean", "")
        score = sum(text.split().count(w) for w in words)
        if score > 0:
            scored.append((score, it))
    scored.sort(key=lambda x: -x[0])
    return [it for _, it in scored[:top_k]]

query2 = "python release"
ranked = search_ranked(items_norm, query2, top_k=5)
print("Consulta:", repr(query2))
for i, it in enumerate(ranked, 1):
    title = it.get("title", "")[:55]
    print(i, "-", title)
#+end_src

** Interfaz de resultados
  
En un IRS la *interfaz* es el medio por el que el usuario formula la consulta
y *ve los resultados*. Sin una presentaci√≥n clara de los documentos recuperados,
el sistema no es usable. En un buscador web t√≠pico la interfaz muestra t√≠tulo,
enlace, fecha y a veces un snippet. En nuestro flujo con feeds RSS y Org-mode,
la ‚Äúpantalla‚Äù de resultados puede ser una *tabla Org*: cada fila es un √≠tem
recuperado (por b√∫squeda booleana o rankeada), con columnas como n√∫mero de
orden, t√≠tulo, URL y fecha. As√≠ se cierra el ciclo IRS: colecci√≥n ‚Üí consulta ‚Üí
recuperaci√≥n ‚Üí *presentaci√≥n* al usuario.

- *IRS ‚Äî Interfaz*: la tabla generada es la *vista de resultados* del sistema;
  el usuario ve qu√© documentos se recuperaron y puede acceder al enlace (RSS
  ~link~) de cada √≠tem.
- *RSS*: los campos mostrados (t√≠tulo, enlace, fecha) vienen directamente del
  feed; el orden de las filas viene del *modelo de recuperaci√≥n* (ranking o
  filtro booleano).
- *C√≥digo*: ~results_to_org_table~ transforma la lista de √≠tems (salida de
  ~search_ranked~ o ~search_boolean_and~) en l√≠neas de tabla Org.

results_to_org_table(items_list)~ recibe la lista de √≠tems ya recuperados
(ordenados por score si vienen de ~search_ranked~). Construye la cabecera de
la tabla Org (~| # | T√≠tulo | Enlace | Fecha |~) y el separador (~|---+...~).
Para cada √≠tem a√±ade una fila: posici√≥n (~i~), t√≠tulo truncado a 50 caracteres,
~link~ (URL del √≠tem en el feed) y fecha truncada a 20 caracteres. El resultado
es una cadena con las l√≠neas de la tabla; en Org-mode se puede insertar en el
buffer o exportar. As√≠ el documento Org act√∫a como ‚Äúpantalla‚Äù del IRS: el usuario
ve el listado rankeado de √≠tems RSS con enlaces clicables.

#+begin_src python :session irs :results value :exports both
def results_to_org_table(items_list):
    """Genera l√≠neas de tabla Org (| col1 | col2 | col3 |)."""
    lines = ["| # | T√≠tulo | Enlace | Fecha |", "|---+--------+--------+-------|"]
    for i, it in enumerate(items_list, 1):
        if "error" in it:
            continue
        title = (it.get("title", "") or "")[:50]
        link = it.get("link", "")
        pub = (it.get("published", "") or "")[:20]
        lines.append(f"| {i} | {title} | {link} | {pub} |")
    return "\n".join(lines)

org_table = results_to_org_table(ranked)
print(org_table)
#+end_src

** Pipeline completo del IRS con RSS

Un IRS real encadena varias etapas: obtener la colecci√≥n, indexar/normalizar,
recibir la consulta, aplicar el modelo de recuperaci√≥n y presentar los
resultados. Integrar todo en un *pipeline* (una sola funci√≥n o flujo) permite
ver de un vistazo c√≥mo se relacionan los componentes del IRS con los feeds RSS:
URL del feed ‚Üí colecci√≥n; consulta del usuario ‚Üí proceso de recuperaci√≥n;
lista rankeada ‚Üí interfaz (tabla). As√≠ se evidencia que el IRS no es solo
‚Äúb√∫squeda‚Äù, sino *colecci√≥n + consulta + modelo + interfaz*.

*** Relaci√≥n IRS ‚Üî feeds RSS en el pipeline
| Componente IRS      | En el pipeline con RSS                          |
|---------------------+-------------------------------------------------|
| Colecci√≥n           | ~fetch_rss(feed_url)~ ‚Üí √≠tems del feed          |
| Normalizaci√≥n       | ~normalize_items(raw)~ ‚Üí ~text_clean~ por √≠tem |
| Consulta            | Par√°metro ~query~ (cadena del usuario)           |
| Modelo recuperaci√≥n | ~search_ranked(normalized, query, top_k)~       |
| Interfaz            | ~results_to_org_table(resultados)~ ‚Üí tabla Org  |

*** Explicaci√≥n del c√≥digo
~irs_pipeline(feed_url, query, max_entries, top_k)~ encadena: (1) ~fetch_rss~
descarga y parsea el feed y devuelve la lista de √≠tems (colecci√≥n en bruto);
(2) si hay error, se devuelve el mensaje; (3) ~normalize_items~ a√±ade
~text_clean~ a cada √≠tem; (4) ~search_ranked~ aplica el modelo de recuperaci√≥n
(ranking por conteo de t√©rminos) y devuelve los ~top_k~ √≠tems m√°s relevantes;
(5) opcionalmente se pasa esa lista a ~results_to_org_table~ para generar la
vista. Una sola llamada con URL de feed y consulta produce la tabla de
resultados que el usuario ver√≠a en la interfaz. As√≠ queda expl√≠cito el flujo
completo: feed RSS ‚Üí colecci√≥n ‚Üí normalizaci√≥n ‚Üí consulta ‚Üí ranking ‚Üí
presentaci√≥n.

#+begin_src python :session irs :results output :exports both
def irs_pipeline(feed_url, query, max_entries=20, top_k=5):
    """Pipeline: fetch ‚Üí normalizar ‚Üí b√∫squeda rankeada ‚Üí resultados."""
    raw = fetch_rss(feed_url, max_entries=max_entries)
    if raw and "error" in raw[0]:
        return raw
    normalized = normalize_items(raw)
    results = search_ranked(normalized, query, top_k=top_k)
    return results

url_ejemplo = "https://pyfound.blogspot.com/feeds/posts/default?alt=rss"
consulta = "python foundation"
resultados = irs_pipeline(url_ejemplo, consulta, max_entries=15, top_k=5)
print("Feed:", url_ejemplo)
print("Consulta:", consulta)
print(results_to_org_table(resultados))
#+end_src



* Introducci√≥n a XPath

*XPath* (XML Path Language) es un lenguaje para localizar nodos y extraer valores en
documentos XML y HTML. Se usa en transformaciones XSLT, scraping web, APIs y consultas
a documentos estructurados.

** ¬øPara qu√© sirve?
- Navegar por la estructura de un √°rbol XML/HTML.
- Seleccionar elementos por etiqueta, atributo, posici√≥n o relaci√≥n jer√°rquica.
- Extraer texto, atributos o fragmentos concretos.
- Usarse desde Python (=lxml=, =BeautifulSoup=), JavaScript (=document.evaluate=), etc.

** Documento de ejemplo

En esta parte usaremos este XML ficticio como referencia:

#+BEGIN_SRC xml
<?xml version="1.0" encoding="UTF-8"?>
<catalogo>
  <libro id="1" categoria="ficcion">
    <titulo>Don Quijote</titulo>
    <autor>Cervantes</autor>
    <precio moneda="EUR">12.50</precio>
    <anio>1605</anio>
  </libro>
  <libro id="2" categoria="tecnica">
    <titulo>Introducci√≥n a XPath</titulo>
    <autor>Ana Garc√≠a</autor>
    <precio moneda="USD">25.00</precio>
    <anio>2023</anio>
  </libro>
  <libro id="3" categoria="ficcion">
    <titulo>Cien a√±os de soledad</titulo>
    <autor>Garc√≠a M√°rquez</autor>
    <precio moneda="EUR">14.00</precio>
    <anio>1967</anio>
  </libro>
</catalogo>
#+END_SRC

** Sintaxis b√°sica

*** Rutas absolutas y relativas

**** Ruta absoluta
Empieza en la ra√≠z del documento con =/=:

#+BEGIN_EXAMPLE
/catalogo/libro
#+END_EXAMPLE

Selecciona todos los elementos =<libro>= que son hijos directos de =<catalogo>=.

**** Ruta relativa
Se eval√∫a desde el nodo actual (contexto):

#+BEGIN_EXAMPLE
libro/titulo
#+END_EXAMPLE

Desde el contexto actual, selecciona =<titulo>= dentro de =<libro>=.

*** Seleccionar por etiqueta

| XPath           | Descripci√≥n                        |
|-----------------+------------------------------------|
| =/catalogo=       | Elemento ra√≠z =catalogo=             |
| =/catalogo/libro= | Todos los =libro= hijos de =catalogo=  |
| =//libro=         | Todos los =libro= en cualquier nivel |
| =//titulo=        | Todos los =titulo= en el documento   |

**** Diferencia entre =/= y =//=
- =/= :: Solo hijos *directos* (un nivel).
- =//= :: Descendientes en *cualquier nivel*.

#+BEGIN_EXAMPLE
/catalogo/libro/titulo   ‚Üí titulos que son nietos de catalogo (1 nivel)
//titulo                 ‚Üí todos los titulos del documento
#+END_EXAMPLE

** Predicados
:PROPERTIES:
:UNNUMBERED: t
:END:

Los *predicados* =[...]= filtran nodos seg√∫n una condici√≥n.

*** Por posici√≥n

| XPath                    | Descripci√≥n                    |
|--------------------------+--------------------------------|
| =/catalogo/libro[1]=     | Primer =libro= (√≠ndice desde 1) |
| =/catalogo/libro[last()]=| √öltimo =libro=                 |
| =/catalogo/libro[position()<=2]= | Los dos primeros       |

**** Nota sobre √≠ndices
En XPath, los √≠ndices empiezan en *1*, no en 0.

*** Por atributo

| XPath                           | Descripci√≥n                          |
|---------------------------------+--------------------------------------|
| =//libro[@id]=                  | Libros que tienen atributo =id=      |
| =//libro[@id="2"]=              | Libro con =id="2"=                   |
| =//libro[@categoria="ficcion"]= | Libros de categor√≠a ficci√≥n          |
| =//precio[@moneda="EUR"]=       | Precios en euros                     |
| =//libro[@id>1]=                | Libros con id num√©rico mayor que 1   |

*** Por contenido de texto

| XPath                           | Descripci√≥n                          |
|---------------------------------+--------------------------------------|
| =//titulo[text()="Don Quijote"]= | T√≠tulo con ese texto exacto         |
| =//titulo[contains(., "a√±os")]=  | T√≠tulos que contienen "a√±os"        |
| =//autor[starts-with(., "Gar")]= | Autores que empiezan por "Gar"      |
| =//libro[precio>13]=             | Libros cuyo precio es mayor que 13  |

El punto =.= representa el nodo actual (el texto del elemento en muchos contextos).

** Ejes
:PROPERTIES:
:UNNUMBERED: t
:END:

Los *ejes* definen la relaci√≥n entre nodos. Sintaxis: =eje::paso=.

*** Ejes m√°s usados

| Eje                 | Abreviatura   | Descripci√≥n                     |
|---------------------+---------------+---------------------------------|
| =child::=             | (por defecto) | Hijos directos                  |
| =descendant::=        | =//=            | Descendientes (cualquier nivel) |
| =parent::=            | =../=           | Padre                           |
| =ancestor::=          |               | Antepasados                     |
| =following-sibling::= |               | Hermanos siguientes             |
| =preceding-sibling::= |               | Hermanos anteriores             |
| =attribute::=         | =@=             | Atributos                       |

*** Ejemplos con ejes

#+BEGIN_EXAMPLE
/catalogo/child::libro          ‚Üí igual que /catalogo/libro
//titulo/parent::libro          ‚Üí libros que contienen un titulo
//libro[1]/following-sibling::libro  ‚Üí hermanos siguientes del primer libro
//libro/@id                     ‚Üí atributo id de cada libro
#+END_EXAMPLE

** Funciones √∫tiles
:PROPERTIES:
:UNNUMBERED: t
:END:

*** Funciones de texto

| Funci√≥n              | Uso                      | Ejemplo                              |
|----------------------+--------------------------+--------------------------------------|
| =text()=             | Texto del nodo           | =//titulo/text()=                    |
| =normalize-space(s)= | Quita espacios extra     | =normalize-space(//titulo[1])=       |
| =contains(s, sub)=   | ¬øs contiene sub?         | =//titulo[contains(., "Quijote")]=   |
| =starts-with(s, pre)=| ¬øs empieza por pre?      | =//autor[starts-with(., "C")]=       |
| =substring(s, i, n)= | Subcadena desde i, n chars | =substring(//titulo[1], 1, 3)=    |
| =concat(s1, s2)=     | Concatenar strings       | =concat(//titulo[1], " - ", //autor[1])= |

*** Funciones num√©ricas

| Funci√≥n        | Uso                    |
|----------------+------------------------|
| =count(nodeset)= | N√∫mero de nodos      |
| =sum(nodeset)=   | Suma de valores      |
| =position()=     | Posici√≥n en el contexto |
| =last()=         | √öltima posici√≥n      |

#+BEGIN_EXAMPLE
count(//libro)           ‚Üí 3
sum(//precio)            ‚Üí 51.50 (si los precios son n√∫meros)
//libro[position()=last()]
#+END_EXAMPLE

*** Funciones de tipo

| Funci√≥n      | Uso                         |
|--------------+-----------------------------|
| =name()=     | Nombre de la etiqueta       |
| =local-name()= | Nombre local (sin prefijo) |
| =string-length(s)= | Longitud de la cadena   |

** Operadores y combinaciones
:PROPERTIES:
:UNNUMBERED: t
:END:

*** Operadores l√≥gicos

#+BEGIN_EXAMPLE
//libro[@categoria="ficcion" and precio>12]
//libro[@categoria="ficcion" or @categoria="tecnica"]
//titulo[not(contains(., "a√±os"))]
#+END_EXAMPLE

*** Uni√≥n de conjuntos (|)

#+BEGIN_EXAMPLE
//titulo | //autor
#+END_EXAMPLE

Devuelve todos los elementos =titulo= y =autor=.

*** Operadores de comparaci√≥n

= = =, =!= =, =< =, =<= =, => =, =>= =

** XPath en HTML (scraping)
:PROPERTIES:
:UNNUMBERED: t
:END:

HTML no siempre es XML bien formado. Herramientas como =lxml= en Python pueden parsear
HTML y aplicar XPath. Ejemplo:

#+BEGIN_SRC python
from lxml import etree

html = """
<html>
  <body>
    <div class="articulo">
      <h1>T√≠tulo del art√≠culo</h1>
      <p>P√°rrafo uno.</p>
      <p>P√°rrafo dos con <a href="/enlace">enlace</a>.</p>
    </div>
    <div class="articulo">
      <h1>Otro art√≠culo</h1>
      <p>Contenido.</p>
    </div>
  </body>
</html>
"""

tree = etree.HTML(html)

# Todos los h1
titulos = tree.xpath("//h1/text()")
print(titulos)  # ['T√≠tulo del art√≠culo', 'Otro art√≠culo']

# h1 dentro de div con clase "articulo"
titulos_articulo = tree.xpath("//div[@class='articulo']/h1/text()")

# Atributo href de enlaces
enlaces = tree.xpath("//a/@href")
print(enlaces)  # ['/enlace']

# Segundo p√°rrafo del primer articulo
segundo_p = tree.xpath("//div[@class='articulo'][1]/p[2]/text()")
#+END_SRC

*** XPath √∫tiles para HTML com√∫n

#+BEGIN_EXAMPLE
//a/@href                    ‚Üí todos los enlaces (URL)
//img/@src                   ‚Üí todas las im√°genes
//div[@class='contenido']    ‚Üí divs con esa clase
//table//tr                  ‚Üí filas de tablas
//input[@name='email']       ‚Üí input por nombre
//*[@id='principal']         ‚Üí elemento con id
#+END_EXAMPLE

** XPath en Python (lxml)
:PROPERTIES:
:UNNUMBERED: t
:END:

*** Cargar XML desde archivo

#+BEGIN_SRC python
from lxml import etree

tree = etree.parse("catalogo.xml")
root = tree.getroot()

# O desde string
xml_string = "<catalogo><libro><titulo>X</titulo></libro></catalogo>"
root = etree.fromstring(xml_string.encode("utf-8"))
#+END_SRC

*** Ejecutar XPath

#+BEGIN_SRC python
# .xpath() devuelve lista de nodos o valores seg√∫n la expresi√≥n
libros = root.xpath("//libro")
titulos = root.xpath("//titulo/text()")
precio_eur = root.xpath("//precio[@moneda='EUR']/text()")
primer_autor = root.xpath("//autor[1]/text()")[0]
#+END_SRC

*** Namespaces (XML con prefijos)

Si el XML usa namespaces (p. ej. =xmlns=), hay que registrarlos:

#+BEGIN_SRC python
ns = {"rss": "http://purl.org/rss/1.0/"}
items = root.xpath("//rss:item/title/text()", namespaces=ns)
#+END_SRC

** Ejercicios pr√°cticos

Con el XML de ejemplo del cat√°logo, escribe el XPath que:

**** Ejercicio 1
Obtenga el t√≠tulo del segundo libro.
#+BEGIN_EXAMPLE
# Respuesta: /catalogo/libro[2]/titulo/text()
#+END_EXAMPLE

**** Ejercicio 2
Obtenga todos los precios en euros.
#+BEGIN_EXAMPLE
# Respuesta: //precio[@moneda='EUR']/text()
#+END_EXAMPLE

**** Ejercicio 3
Obtenga los t√≠tulos de los libros de ficci√≥n.
#+BEGIN_EXAMPLE
# Respuesta: //libro[@categoria='ficcion']/titulo/text()
#+END_EXAMPLE

**** Ejercicio 4
Obtenga el autor del libro cuyo t√≠tulo contiene "a√±os".
#+BEGIN_EXAMPLE
# Respuesta: //libro[titulo[contains(., 'a√±os')]]/autor/text()
#+END_EXAMPLE

**** Ejercicio 5
Cuente cu√°ntos libros hay en el cat√°logo.
#+BEGIN_EXAMPLE
# Respuesta: count(//libro)
#+END_EXAMPLE

** Referencia r√°pida
:PROPERTIES:
:UNNUMBERED: t
:END:

| S√≠mbolo / sintaxis | Significado              |
|--------------------+--------------------------|
| =/=                | Hijo directo / ra√≠z      |
| =//=               | Descendiente (cualquier nivel) |
| =.=                | Nodo actual              |
| =..=               | Padre                    |
| =@=                | Atributo                 |
| =[]=               | Predicado (filtro)       |
| =|=                | Uni√≥n de conjuntos       |
| =text()=           | Contenido de texto       |
| =*=                | Cualquier elemento       |
| =@*=               | Cualquier atributo       |

** Dependencias
:PROPERTIES:
:UNNUMBERED: t
:END:

#+BEGIN_EXAMPLE
pip install lxml
#+END_EXAMPLE

Para HTML "suelto": =lxml= con =etree.HTML()= o =BeautifulSoup= con soporte =lxml=.








*  Ruta Cr√≠tica del Web Scraping y Aspectos de Seguridad




** Introducci√≥n

Este tutorial describe la *ruta cr√≠tica* para realizar web scraping: los pasos imprescindibles,
orden l√≥gico y las consideraciones de *seguridad* (tuyas y del sitio objetivo) que conlleva.

** La ruta cr√≠tica: fases del proyecto de scraping
:PROPERTIES:
:UNNUMBERED: t
:END:

La ruta cr√≠tica es la secuencia de actividades que deben completarse en orden y cuyas
demoras afectan directamente el tiempo total del proyecto.

*** Diagrama de flujo general

#+BEGIN_EXAMPLE
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. LEGALIDAD    ‚îÇ ¬øPuedo hacerlo?
‚îÇ y √âTICA         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ S√≠
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. ALTERNATIVAS ‚îÇ ¬øExiste API o descarga oficial?
‚îÇ (evitar scrape) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ No hay alternativa adecuada
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. PLANIFICACI√ìN‚îÇ ¬øQu√© datos? ¬øQu√© p√°ginas? ¬øFrecuencia?
‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. AN√ÅLISIS     ‚îÇ Inspeccionar HTML, estructura, paginaci√≥n
‚îÇ T√âCNICO         ‚îÇ ¬øContenido est√°tico o din√°mico?
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. DESARROLLO   ‚îÇ C√≥digo: requests/lxml/selenium, etc.
‚îÇ                 ‚îÇ Manejo de errores, reintentos
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 6. SEGURIDAD    ‚îÇ Headers, rate limiting, proxies (si aplica)
‚îÇ Y ROBUSTEZ      ‚îÇ Validaci√≥n de datos, sanitizaci√≥n
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 7. EJECUCI√ìN    ‚îÇ Monitoreo, logs, alertas
‚îÇ Y MANTENIMIENTO ‚îÇ Adaptaci√≥n a cambios del sitio
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
#+END_EXAMPLE

** Fase 1: Legalidad y √©tica (puerta de entrada)
:PROPERTIES:
:UNNUMBERED: t
:END:

*** Antes de escribir c√≥digo

| Verificaci√≥n | Pregunta | Acci√≥n si falla |
|--------------+----------+-----------------|
| T√©rminos de uso | ¬øEl sitio proh√≠be expl√≠citamente el scraping? | No proceder |
| robots.txt | ¬øHay directivas que bloqueen tu user-agent o rutas? | Respetar o no proceder |
| Propiedad intelectual | ¬øEl contenido est√° protegido por derechos de autor? | Evaluar uso justo / licencia |
| Datos personales | ¬øVas a extraer datos que identifiquen personas? | Cumplir RGPD/LFPDPPP y normativa local |
| robots.txt location | ¬øD√≥nde est√°? | Revisar =https://dominio.com/robots.txt= |

*** Ejemplo de robots.txt

#+BEGIN_EXAMPLE
User-agent: *
Disallow: /admin/
Disallow: /api/  # Puede indicar que hay API disponible
Allow: /public/

User-agent: BadBot
Disallow: /
#+END_EXAMPLE

*** Consecuencias de omitir esta fase

- Demandas por violaci√≥n de t√©rminos de servicio.
- Bloqueo de IP o cuenta.
- Responsabilidad por tratamiento indebido de datos personales.
- Reputaci√≥n y costes legales.

** Fase 2: Buscar alternativas al scraping
:PROPERTIES:
:UNNUMBERED: t
:END:

*** Jerarqu√≠a de preferencia (orden recomendado)

1. *API oficial* (Twitter, INEGI, Amazon Product Advertising API, etc.)
2. *RSS / feed XML*
3. *JSON oculto en Network* (pesta√±a Network del navegador: peticiones XHR/Fetch que devuelven JSON)
4. *HTML est√°tico* (=requests= + lxml)
5. *Contenido din√°mico* (Selenium, Playwright)

Nunca empezar con Selenium si no es necesario: es m√°s lento y pesado. Ejemplos: Twitter/X
‚Üí API oficial (plan b√°sico limitado); Amazon ‚Üí Product Advertising API para afiliados.

*** Antes de scrapear, preguntar

1. ¬øExiste una *API oficial*? (Google, Twitter, INEGI, etc.)
2. ¬øHay *datos abiertos* o descarga masiva? (datos.gob.mx, Kaggle)
3. ¬øOfrecen *exportaci√≥n* o *feed RSS/XML*?
4. ¬øEl sitio carga datos por JSON interno? (revisar pesta√±a Network)
5. ¬øSe puede *contactar* al propietario para solicitar acceso formal?

*** Motivos para preferir alternativas

- Legalidad m√°s clara.
- Menor riesgo de bloqueo.
- Datos m√°s estructurados y estables.
- Menos mantenimiento ante cambios del sitio.

** Fase 3: Planificaci√≥n
:PROPERTIES:
:UNNUMBERED: t
:END:

*** Definir alcance

| Aspecto | Preguntas |
|---------+-----------|
| Objetivo | ¬øQu√© datos exactos necesito? |
| Volumen | ¬øCu√°ntas p√°ginas/registros? |
| Frecuencia | ¬øUna vez, diario, en tiempo real? |
| Formato salida | CSV, JSON, base de datos |
| Tiempo estimado | ¬øCu√°nto tardar√° una ejecuci√≥n completa? |

*** Estimar carga al servidor

\[
\text{peticiones/hora} \approx \frac{\text{p√°ginas totales}}{\text{tiempo deseado (h)}} \times (1 + \text{margen reintentos})
\]

Mantener un ritmo *razonable* evita saturar el servidor y ser detectado como bot.

** Fase 4: An√°lisis t√©cnico
:PROPERTIES:
:UNNUMBERED: t
:END:

*** Inspeccionar el sitio

- Herramientas de desarrollador (F12): estructura HTML, clases, IDs.
- Network tab: ver peticiones XHR/Fetch (contenido din√°mico).
- Probar si =requests= + =lxml= devuelven el contenido visible en el navegador.

*** Contenido est√°tico vs. din√°mico

| Tipo | C√≥mo identificarlo | Herramienta t√≠pica |
|------+--------------------+---------------------|
| Est√°tico | HTML completo en la respuesta GET | =requests= + =lxml= |
| Din√°mico | Contenido cargado por JavaScript | Selenium, Playwright |

*** Mapear la estructura

- URLs base y patrones de paginaci√≥n.
- Selectores XPath o CSS estables.
- Formularios, cookies o cabeceras requeridas.

*** Detectando JSON oculto (Network)

Muchos sitios (incl. Amazon, tiendas, portales) cargan datos por peticiones internas a APIs
que devuelven JSON. En lugar de parsear HTML:

1. Abrir herramientas de desarrollador (F12) ‚Üí pesta√±a *Network*.
2. Filtrar por XHR o Fetch.
3. Recargar la p√°gina y observar qu√© peticiones devuelven datos √∫tiles.
4. Copiar la URL de la petici√≥n y sus par√°metros; replicar con =requests=.

Ventaja: JSON estructurado, m√°s f√°cil de parsear y m√°s estable que el HTML.

** Fase 5: Desarrollo
:PROPERTIES:
:UNNUMBERED: t
:END:

*** Estructura recomendada del c√≥digo

#+BEGIN_EXAMPLE
scraper/
‚îú‚îÄ‚îÄ config.py        # URLs, timeouts, headers
‚îú‚îÄ‚îÄ fetch.py         # L√≥gica de peticiones HTTP
‚îú‚îÄ‚îÄ parse.py         # Extracci√≥n con XPath/CSS
‚îú‚îÄ‚îÄ storage.py       # Guardar datos (CSV, DB)
‚îú‚îÄ‚îÄ main.py          # Orquestaci√≥n
‚îî‚îÄ‚îÄ logs/            # Registro de ejecuciones
#+END_EXAMPLE

*** Manejo de errores

#+BEGIN_SRC python
import time
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def session_con_reintentos():
    session = requests.Session()
    retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503])
    session.mount("https://", HTTPAdapter(max_retries=retries))
    return session

def fetch_con_pausa(url, session, delay=1):
    try:
        r = session.get(url, timeout=30)
        r.raise_for_status()
        time.sleep(delay)  # Rate limiting
        return r.content
    except requests.RequestException as e:
        # Registrar en log, no solo print
        raise
#+END_SRC

** Fase 6: Seguridad y robustez
:PROPERTIES:
:UNNUMBERED: t
:END:

*** Seguridad de las peticiones

**** User-Agent e identificaci√≥n

- Identificarse de forma clara (no hacerse pasar por Googlebot si no lo eres).
- Incluir un correo de contacto si el sitio lo sugiere.
- Evitar user-agents de navegadores muy antiguos (suelen ser bloqueados).

#+BEGIN_SRC python
HEADERS = {
    "User-Agent": "MiProyecto/1.0 (estudio acad√©mico; +mail@ejemplo.com)",
    "Accept": "text/html,application/xhtml+xml",
    "Accept-Language": "es-ES,es;q=0.9",
}
#+END_SRC

**** No exponer credenciales

- Nunca hardcodear API keys, contrase√±as o tokens.
- Usar variables de entorno o archivos de configuraci√≥n excluidos de control de versiones.

#+BEGIN_SRC python
import os
API_KEY = os.environ.get("SCRAPER_API_KEY")
if not API_KEY:
    raise ValueError("Configurar SCRAPER_API_KEY")
#+END_SRC

**** Validar y sanitizar datos de entrada

- URLs, par√°metros y consultas pueden venir de fuentes externas.
- Evitar inyecci√≥n o URLs maliciosas (redirecciones a dominios desconocidos).

#+BEGIN_SRC python
from urllib.parse import urlparse

def url_permitida(url, dominios_permitidos):
    parsed = urlparse(url)
    return parsed.netloc in dominios_permitidos
#+END_SRC

*** Seguridad de los datos extra√≠dos

**** Sanitizaci√≥n de salida

- Limpiar HTML antes de guardar (evitar XSS si luego se muestran en una web).
- Validar tipos y formatos (fechas, n√∫meros, textos).

#+BEGIN_SRC python
import re
def limpiar_texto(texto):
    if not isinstance(texto, str):
        return ""
    return re.sub(r"<[^>]+>", "", texto).strip()
#+END_SRC

**** Almacenamiento

- No guardar datos personales innecesarios.
- Cifrar almacenamiento si hay datos sensibles.
- Respaldar solo lo necesario y cumplir normativa de retenci√≥n.

*** Protecci√≥n del sitio objetivo (√©tica t√©cnica)

| Pr√°ctica | Objetivo |
|----------|----------|
| Rate limiting | No saturar el servidor; pausas entre peticiones |
| Horarios valle | Ejecutar en horas de menor tr√°fico si es posible |
| Cache | Evitar repetir peticiones innecesarias |
| Respetar 429/503 | Detener o ralentizar si el servidor pide esperar |
| No evadir bloqueos | Si te bloquean, revisar si tu uso es adecuado |

*** Proxies y escalado (consideraciones)

- Proxies rotativos: mayor complejidad y coste; puede violar t√©rminos de uso.
- Antes de usarlos: confirmar legalidad y pol√≠tica del sitio.
- Preferir optimizar el ritmo antes que multiplicar IPs.

** Fase 7: Ejecuci√≥n y mantenimiento
:PROPERTIES:
:UNNUMBERED: t
:END:

*** Logging

#+BEGIN_SRC python
import logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)
logger.info("Iniciando scraping de %s", url)
logger.warning("Reintento %d para %s", intento, url)
logger.error("Error en %s: %s", url, str(e))
#+END_SRC

*** Monitoreo b√°sico

- Tasa de √©xito vs. errores.
- Tiempo de respuesta y c√≥digos HTTP.
- Cambios en la estructura (parsers que dejan de funcionar).

*** Adaptaci√≥n a cambios

- Los sitios modifican HTML, clases y rutas.
- Dise√±ar selectores robustos (evitar dependencias fr√°giles).
- Revisar peri√≥dicamente que el scraper sigue funcionando.

** Checklist de seguridad resumido
:PROPERTIES:
:UNNUMBERED: t
:END:

| # | Verificaci√≥n | Estado |
|---|--------------+--------|
| 1 | T√©rminos de uso y robots.txt revisados | ‚òê |
| 2 | Sin API/alternativa viable | ‚òê |
| 3 | Credenciales en variables de entorno | ‚òê |
| 4 | User-Agent identificable | ‚òê |
| 5 | Rate limiting / pausas implementados | ‚òê |
| 6 | Manejo de errores y reintentos | ‚òê |
| 7 | Validaci√≥n de URLs y dominios permitidos | ‚òê |
| 8 | Sanitizaci√≥n de datos extra√≠dos | ‚òê |
| 9 | Sin almacenar datos personales innecesarios | ‚òê |
| 10 | Logging y monitoreo b√°sico | ‚òê |

** Riesgos y mitigaciones
:PROPERTIES:
:UNNUMBERED: t
:END:

| Riesgo | Mitigaci√≥n |
|--------+------------|
| Bloqueo de IP | Rate limiting, horarios valle, User-Agent adecuado |
| Cambio de estructura HTML | Selectores resilientes, pruebas peri√≥dicas |
| Exposici√≥n de credenciales | Variables de entorno, .gitignore |
| Datos personales | Minimizaci√≥n, anonimizaci√≥n, cumplimiento legal |
| Ca√≠da del servidor objetivo | Reintentos con backoff, detecci√≥n de 429/503 |
| Malware en respuestas | Validar tipos MIME, no ejecutar c√≥digo descargado |
| Sanciones legales | Cumplir ToS, robots.txt y normativa aplicable |

** Referencias
:PROPERTIES:
:UNNUMBERED: t
:END:

- ~Tutorial_Webscraping_XPath_Blogs.org~: T√©cnicas de scraping con XPath, ejemplo likcos.
- ~Fuentes_Datos_Gobierno_Abierto.org~: INEGI, datos.gob.mx, clima, alternativas legales.
- ~Tutorial_XPath.org~: Sintaxis XPath y comparaci√≥n XPath vs CSS.
- robots.txt: =https://dominio.com/robots.txt=
- RGPD / LFPDPPP: normativa aplicable a datos personales seg√∫n jurisdicci√≥n.

* Web Scraping con XPath para Blogs Est√°ticos

** Introducci√≥n
:PROPERTIES:
:UNNUMBERED: t
:END:

Este apartado explica c√≥mo hacer *web scraping* con XPath sobre *blogs est√°ticos* (HTML
pre-renderizado en el servidor), qu√© consideraciones t√©cnicas y √©ticas tener, y por qu√©
*sitios como Amazon* no son adecuados para este enfoque.

*** Prerrequisitos

- Python 3 con =requests=, =lxml= (o =BeautifulSoup= con parser lxml).
- Conocimientos b√°sicos de XPath (ver ~Tutorial_XPath.org~).
- Comprensi√≥n de la estructura HTML (etiquetas, clases, ids).

** Blogs est√°ticos vs. sitios din√°micos
:PROPERTIES:
:UNNUMBERED: t
:END:

*** ¬øQu√© es un blog est√°tico?

Un blog est√°tico sirve HTML ya generado por el servidor. Al hacer una petici√≥n GET,
recibes el contenido completo en la respuesta. No necesitas ejecutar JavaScript para
ver los art√≠culos.

**** Caracter√≠sticas t√≠picas
- HTML con estructura predecible (art√≠culos en =<article>=, =<div class="post">=, etc.).
- Paginaci√≥n mediante URLs distintas (=/page/2/=, =?p=2=).
- Contenido p√∫blico y pensado para ser indexado por buscadores.
- Sin capchas agresivos ni medidas anti-bot fuertes (en muchos casos).

*** ¬øPor qu√© son adecuados para scraping?

| Aspecto        | Blogs est√°ticos                    |
|----------------+------------------------------------|
| Contenido      | HTML completo en la respuesta      |
| Herramientas   | =requests= + =lxml= suficientes    |
| Estructura     | Generalmente clara y consistente   |
| Legalidad      | M√°s predecible si es uso personal  |
| robots.txt     | Suele permitir acceso a contenido  |

** T√©cnicas de scraping para blogs con XPath
:PROPERTIES:
:UNNUMBERED: t
:END:

*** Paso 1: Inspeccionar la estructura HTML

Antes de escribir XPath, abre el blog en el navegador y usa las herramientas de
desarrollador (F12) para ver:

- Contenedor de art√≠culos: =<article>=, =<div class="post">=, =<div class="entry">=.
- T√≠tulo: =<h1>=, =<h2>=, =<a class="title">=.
- Autor, fecha, categor√≠as.
- Enlace al art√≠culo completo: =<a href="...">=.
- Snippet o extracto: =<p class="excerpt">=, =<div class="summary">=.

*** Paso 2: Obtener el HTML

#+BEGIN_SRC python
import requests
from lxml import etree

url = "https://ejemplo-blog.com/"
headers = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36"
}
response = requests.get(url, headers=headers)
response.raise_for_status()
html = response.content
#+END_SRC

*** Paso 3: Parsear y aplicar XPath

#+BEGIN_SRC python
tree = etree.HTML(html)

# Ejemplos t√≠picos para blogs (ajustar selectores seg√∫n el sitio):
# Todos los art√≠culos
articulos = tree.xpath("//article")
# o
articulos = tree.xpath("//div[contains(@class, 'post')]")

# T√≠tulo del primer art√≠culo
titulo = tree.xpath("//article[1]//h2/a/text()")[0].strip()

# Enlace al art√≠culo
enlace = tree.xpath("//article[1]//h2/a/@href")[0]

# Fecha (si est√° en un elemento con clase 'date')
fecha = tree.xpath("//article[1]//time/@datetime")
fecha = fecha[0] if fecha else tree.xpath("//article[1]//*[contains(@class,'date')]/text()")

# Extracto o resumen
extracto = tree.xpath("//article[1]//div[contains(@class,'excerpt')]//text()")
extracto = " ".join(t.strip() for t in extracto if t.strip())
#+END_SRC

*** Paso 4: Extraer todos los art√≠culos de una p√°gina

#+BEGIN_SRC python
def extraer_articulos_pagina(html):
    tree = etree.HTML(html)
    articulos = tree.xpath("//article")
    resultado = []
    for art in articulos:
        titulo = art.xpath(".//h2/a/text() | .//h1/a/text() | .//.//a[contains(@class,'title')]/text()")
        enlace = art.xpath(".//h2/a/@href | .//h1/a/@href | .//a[contains(@class,'title')]/@href")
        extracto = art.xpath(".//div[contains(@class,'excerpt')]//text() | .//p[contains(@class,'summary')]//text()")
        resultado.append({
            "titulo": titulo[0].strip() if titulo else "",
            "enlace": enlace[0] if enlace else "",
            "extracto": " ".join(t.strip() for t in extracto if t.strip())[:200]
        })
    return resultado
#+END_SRC

*** Paso 5: Paginaci√≥n

Muchos blogs usan URLs como =/page/2/=, =/?paged=2=, =/blog?page=2=:

#+BEGIN_SRC python
import time

base_url = "https://ejemplo-blog.com/page/{}/"
for pagina in range(1, 6):  # p√°ginas 1 a 5
    url = base_url.format(pagina)
    response = requests.get(url, headers=headers)
    articulos = extraer_articulos_pagina(response.content)
    for a in articulos:
        print(a["titulo"], a["enlace"])
    time.sleep(1)  # respetar el servidor: pausa entre peticiones
#+END_SRC

*** XPath √∫tiles para blogs (resumen)

| Objetivo          | XPath t√≠pico                                           |
|-------------------+--------------------------------------------------------|
| Contenedor posts  | =//article=, =//div[contains(@class,'post')]=          |
| T√≠tulo            | =.//h1/a/text()=, =.//h2/a/text()=                     |
| Enlace            | =.//h1/a/@href=, =.//a[contains(@class,'entry-link')]/@href= |
| Fecha             | =.//time/@datetime=, =.//*[contains(@class,'date')]/text()= |
| Autor             | =.//*[contains(@class,'author')]/text()=               |
| Categor√≠as        | =.//a[contains(@class,'category')]/text()=             |
| Extracto          | =.//div[contains(@class,'excerpt')]//text()=           |

** Consideraciones √©ticas y legales

*** Buenas pr√°cticas

1. *robots.txt*: Revisar si el sitio proh√≠be o limita el scraping.
2. *User-Agent*: Identificarse de forma razonable (no hacerse pasar por bot de Google).
3. *Rate limiting*: Hacer pausas entre peticiones (=time.sleep(1)= o m√°s).
4. *Uso razonable*: Evitar sobrecargar el servidor; respetar t√©rminos de uso.
5. *Atribuci√≥n*: Si redistribuyes contenido, cita la fuente.

*** Cu√°ndo es m√°s aceptable

- Blogs personales o peque√±os con contenido p√∫blico.
- Uso educativo o de investigaci√≥n.
- Agregadores de noticias que respetan robots.txt y t√©rminos de uso.
- Contenido bajo licencias abiertas (CC, etc.).

** Por qu√© NO es adecuado hacer scraping en Amazon (y similares)
:PROPERTIES:
:UNNUMBERED: t
:END:

*** 1. T√©rminos de uso y condiciones

Amazon (y la mayor√≠a de grandes plataformas) *proh√≠ben expl√≠citamente* el scraping en
sus condiciones de servicio. Hacerlo puede suponer:

- Suspensi√≥n o cierre de cuenta.
- Acciones legales.
- Bloqueo de IP.

No es una limitaci√≥n t√©cnica, sino contractual y legal.

*** 2. Contenido din√°mico (JavaScript)

Amazon carga gran parte del contenido mediante JavaScript:

- Listados de productos se actualizan con peticiones AJAX.
- Precios, disponibilidad y valoraciones se insertan despu√©s del HTML inicial.
- Una petici√≥n con =requests= devuelve HTML casi vac√≠o respecto a lo que ves en el navegador.

Para scraping efectivo har√≠a falta un navegador headless (Selenium, Playwright), lo que:

- Es m√°s complejo y pesado.
- Genera m√°s carga para el servidor.
- Suele detectarse m√°s f√°cilmente como bot.

*** 3. Medidas anti-bot

| Medida          | Descripci√≥n                                  |
|-----------------+----------------------------------------------|
| Rate limiting   | Bloqueo temporal tras muchas peticiones      |
| CAPTCHAs        | Verificaciones que rompen automatizaci√≥n     |
| Fingerprinting  | Detecci√≥n de patrones de navegaci√≥n          |
| IP blocking     | Bloqueo de IPs sospechosas                   |
| Contenido ofuscado | Clases/ids aleatorios que cambian           |

*** 4. Estructura cambiante

- El HTML de Amazon cambia con frecuencia (A/B tests, redise√±os).
- Selectores XPath que funcionan hoy pueden dejar de hacerlo en semanas.
- Mantener un scraper estable es costoso y fr√°gil.

*** 5. Alternativas legales y recomendadas

- *Amazon Product Advertising API*: Para afiliados y partners autorizados.
- *Otros APIs oficiales*: Muchas tiendas ofrecen APIs para precios, inventario, etc.
- *Bases de datos de terceros*: Datasets acad√©micos o comerciales de productos.

*** Resumen: Amazon vs. blog est√°tico

| Aspecto         | Blog est√°tico              | Amazon (y similares)     |
|-----------------+----------------------------+---------------------------|
| T√©rminos de uso | Suele ser tolerable        | Prohibido expl√≠citamente  |
| Contenido       | HTML est√°tico completo     | Mucho contenido din√°mico  |
| Anti-bot        | Escaso o inexistente       | Muy presente              |
| Estructura      | Estable y predecible       | Cambia con frecuencia     |
| Alternativas    | Scraping razonable         | Usar API oficial          |

** Ejemplo completo: scraper para un blog est√°tico
:PROPERTIES:
:UNNUMBERED: t
:END:

#+BEGIN_SRC python
import requests
from lxml import etree
import time
import csv

def scrape_blog(base_url, num_paginas=3):
    """Extrae art√≠culos de un blog est√°tico con paginaci√≥n /page/N/."""
    headers = {"User-Agent": "Mozilla/5.0 (compatible; BlogStudyBot/1.0)"}
    articulos = []
    for p in range(1, num_paginas + 1):
        url = f"{base_url.rstrip('/')}/page/{p}/" if p > 1 else base_url
        try:
            r = requests.get(url, headers=headers, timeout=10)
            r.raise_for_status()
            tree = etree.HTML(r.content)
            for art in tree.xpath("//article"):
                tit = art.xpath(".//h2/a/text() | .//h1/a/text()")
                lnk = art.xpath(".//h2/a/@href | .//h1/a/@href")
                ext = art.xpath(".//div[contains(@class,'excerpt')]//text()")
                articulos.append({
                    "titulo": tit[0].strip() if tit else "",
                    "enlace": lnk[0] if lnk else "",
                    "extracto": " ".join(t.strip() for t in ext if t.strip())[:300]
                })
        except Exception as e:
            print(f"Error en {url}: {e}")
        time.sleep(1)
    return articulos

# Uso (sustituir por un blog real que permita scraping):
# datos = scrape_blog("https://ejemplo-blog.com/", num_paginas=3)
# for a in datos:
#     print(a["titulo"], "->", a["enlace"])
#+END_SRC

** Dependencias

#+BEGIN_EXAMPLE
pip install requests lxml
#+END_EXAMPLE

** Referencias

- ~Tutorial_XPath.org~: Sintaxis y ejemplos de XPath.
- =robots.txt=: Revisar =https://dominio.com/robots.txt= antes de scrapear.
- APIs: Buscar si el sitio ofrece una API oficial antes de automatizar.




*  Fuentes de Datos Gubernamentales y Abiertos: INEGI, datos.gob.mx y m√°s

** Introducci√≥n

Instituciones gubernamentales publican *datos abiertos* bajo marcos legales que permiten
su reutilizaci√≥n. Es preferible usar *APIs oficiales* o *descargas directas* en lugar de
scraping cuando est√°n disponibles: son legales, estables y eficientes.

Este apartado recopila fuentes como *INEGI*, *datos.gob.mx* y otras donde s√≠ podemos
obtener informaci√≥n de forma adecuada.

** INEGI (M√©xico)
:PROPERTIES:
:UNNUMBERED: t
:END:

*** Qu√© es INEGI

El *Instituto Nacional de Estad√≠stica y Geograf√≠a* (INEGI) produce estad√≠sticas y
geograf√≠a de M√©xico. Ofrece varias formas de acceso program√°tico a sus datos.

*** API del Banco de Indicadores

**** Registro y token

- URL: ~https://www.inegi.org.mx/servicios/api_indicadores.html~
- Requiere *registro* para obtener un *token*.
- Registro: ~https://www.inegi.org.mx/app/desarrolladores/registro/default.html~

**** Par√°metros t√≠picos

| Par√°metro         | Descripci√≥n                            | Ejemplo                      |
|-------------------+----------------------------------------+------------------------------|
| IdIndicador       | Clave del indicador                    | 1002000001 (Poblaci√≥n total) |
| Idioma            | es / en                                | es                           |
| √Årea geogr√°fica   | 00=nacional, 99=entidad, 999=municipio | 00                           |
| Dato m√°s reciente | true / false                           | false (serie hist√≥rica)      |
| Fuente            | BISE, etc.                             | BISE                         |
| Token             | Token de registro                      | [tu token]                   |
| Formato           | json, jsonp, xml                       | json                         |

**** Ejemplo en Python (API Indicadores)

#+BEGIN_SRC python
import requests

TOKEN = "TU_TOKEN_AQUI"  # Obtener en inegi.org.mx/app/desarrolladores/registro
# Indicador 1002000001 = Poblaci√≥n total
url = (
    f"https://www.inegi.org.mx/app/api/indicadores/desarrolladores/jsonxml/"
    f"INDICATOR/1002000001/es/00/false/BISE/2.0/{TOKEN}?type=json"
)
response = requests.get(url)
data = response.json()
# Estructura: data["Series"][0]["OBSERVATIONS"]
for obs in data.get("Series", [{}])[0].get("OBSERVATIONS", []):
    print(obs["TIME_PERIOD"], obs["OBS_VALUE"])
#+END_SRC

*** API DENUE (Directorio Estad√≠stico Nacional de Unidades Econ√≥micas)

- URL: ~https://www.inegi.org.mx/servicios/api_denue.html~
- Busca establecimientos por palabra, actividad econ√≥mica, ubicaci√≥n, etc.
- Requiere token.
- Devuelve JSON con unidades econ√≥micas.

**** Ejemplo de consulta DENUE

#+BEGIN_SRC python
TOKEN = "TU_TOKEN"
busqueda = "restaurante"
entidad = "19"  # Nuevo Le√≥n
url = f"https://www.inegi.org.mx/app/api/denue/v1/consulta/buscar/{busqueda}/{entidad}/{TOKEN}"
response = requests.get(url)
establecimientos = response.json()
#+END_SRC

*** Datos Abiertos INEGI

- URL: ~https://www.inegi.org.mx/datosabiertos/~
- Archivos en formatos est√°ndar (CSV, Excel, etc.).
- Descarga directa sin API; suele usarse para conjuntos grandes.

*** Descarga Masiva INEGI

- URL: ~https://www.inegi.org.mx/app/descarga/~
- Permite seleccionar DENUE, Banco de Indicadores, Inventario Nacional de Viviendas,
  microdatos y tabulados por √°rea geogr√°fica, tema, per√≠odo y formato.

*** Librer√≠a inegipy (Python)

#+BEGIN_EXAMPLE
pip install inegipy
#+END_EXAMPLE

Facilita el acceso al Banco de Indicadores, DENUE y Marco Geoestad√≠stico.
Documentaci√≥n: ~https://pypi.org/project/inegipy/~

** datos.gob.mx (Plataforma Nacional de Datos Abiertos)
:PROPERTIES:
:UNNUMBERED: t
:END:

*** Qu√© es datos.gob.mx

Plataforma centralizada de *datos abiertos* del Gobierno de M√©xico. Incluye miles de
bases de datos de dependencias federales, estatales y organismos aut√≥nomos.

- URL: ~https://datos.gob.mx/~
- M√°s de 5 000 bases de datos.
- Categor√≠as: Econom√≠a, Educaci√≥n, Salud, Cultura, Ciencia y Tecnolog√≠a, etc.

*** API CKAN

La plataforma usa *CKAN*. Su API permite listar conjuntos, buscar y obtener URLs de
descarga de recursos (CSV, Excel, etc.).

**** Endpoints √∫tiles

| Acci√≥n | URL | Descripci√≥n |
|--------+-----+-------------|
| Listar datasets | =/api/3/action/package_list= | IDs de todos los conjuntos |
| Ver dataset | =/api/3/action/package_show?id=ID= | Metadatos y recursos |
| Buscar | =/api/3/action/package_search?q=TERMINO= | B√∫squeda por t√©rmino |
| Ver recurso | =/api/3/action/resource_show?id=ID= | Detalles de un recurso |

**** Ejemplo: listar y buscar conjuntos de datos

#+BEGIN_SRC python
import requests

BASE = "https://datos.gob.mx/api/3/action"

# Listar todos los IDs de conjuntos
r = requests.get(f"{BASE}/package_list")
datasets = r.json().get("result", [])

# Buscar por t√©rmino
r = requests.get(f"{BASE}/package_search", params={"q": "educaci√≥n"})
resultados = r.json().get("result", {}).get("results", [])
for d in resultados[:5]:
    print(d["title"], "-", d.get("organization", {}).get("title"))
#+END_SRC

**** Ejemplo: obtener URL de descarga de un recurso

#+BEGIN_SRC python
# Supongamos que tenemos el id del dataset (ej. "catalogo-escuelas")
dataset_id = "catalogo-escuelas"
r = requests.get(f"{BASE}/package_show", params={"id": dataset_id})
pkg = r.json().get("result", {})
for res in pkg.get("resources", []):
    if res.get("format", "").upper() == "CSV":
        url_descarga = res["url"]
        print("Descargar CSV:", url_descarga)
        # response = requests.get(url_descarga)
        # with open("datos.csv", "wb") as f:
        #     f.write(response.content)
        break
#+END_SRC

*** Descarga directa de archivos

En la web de datos.gob.mx cada dataset tiene recursos con enlaces de descarga directa.
Se puede usar la API para obtener esas URLs y luego =requests.get()= para descargar.

** Otras fuentes gubernamentales de datos abiertos

*** M√©xico

| Fuente             | URL            | Descripci√≥n                                  |
|--------------------+----------------+----------------------------------------------|
| INEGI              | inegi.org.mx   | Estad√≠sticas, indicadores, DENUE, geograf√≠a  |
| datos.gob.mx       | datos.gob.mx   | Cat√°logo nacional de datos abiertos          |
| SAT datos abiertos | gob.mx/sat     | Informaci√≥n fiscal y aduanal                 |
| SEPOMEX            | sepomex.gob.mx | C√≥digos postales (consultar t√©rminos de uso) |

*** Espa√±a

| Fuente       | URL          | Descripci√≥n                        |
|--------------+--------------+------------------------------------|
| datos.gob.es | datos.gob.es | Portal de datos abiertos de Espa√±a |

*** Internacional

| Fuente               | URL                   | Descripci√≥n                     |
|----------------------+-----------------------+---------------------------------|
| World Bank Open Data | data.worldbank.org    | Indicadores mundiales           |
| UN Data              | data.un.org           | Estad√≠sticas de Naciones Unidas |
| Eurostat             | ec.europa.eu/eurostat | Estad√≠sticas europeas           |

** Cu√°ndo usar cada m√©todo

| M√©todo           | Cu√°ndo usarlo                                        | Ejemplo                                   |
|------------------+------------------------------------------------------+-------------------------------------------|
| API oficial      | Existe y cubre tus necesidades                       | INEGI Indicadores, DENUE                  |
| Descarga directa | Archivos CSV/Excel en el portal                      | datos.gob.mx, INEGI Datos Abiertos        |
| API CKAN         | Listar, buscar, obtener URLs de recursos             | datos.gob.mx                              |
| Scraping         | Solo si no hay API ni descarga y el sitio lo permite | Evitar en gobierno; preferir alternativas |

** Buenas pr√°cticas

1. *Priorizar API y descargas oficiales*: Son legales, estables y est√°n pensadas para uso program√°tico.
2. *Revisar t√©rminos de uso y licencias*: Aunque sean datos abiertos, pueden haber condiciones.
3. *Identificarse*: Usar un User-Agent razonable si haces peticiones HTTP.
4. *No saturar servidores*: Respetar rate limits; hacer pausas entre peticiones masivas.
5. *Atribuir la fuente*: Citar INEGI, datos.gob.mx, etc., al publicar o reutilizar datos.
6. *Protecci√≥n de datos*: Evitar extraer o combinar datos que permitan identificar personas sin base legal.

** Ejemplo completo: indicador INEGI + guardar CSV

#+BEGIN_SRC python
import requests
import csv

TOKEN = "TU_TOKEN"
# INPC - √çndice Nacional de Precios al Consumidor (ejemplo)
indicador = "628193"  # Ajustar al indicador deseado
url = (
    f"https://www.inegi.org.mx/app/api/indicadores/desarrolladores/jsonxml/"
    f"INDICATOR/{indicador}/es/00/false/BISE/2.0/{TOKEN}?type=json"
)
r = requests.get(url)
data = r.json()

observaciones = data.get("Series", [{}])[0].get("OBSERVATIONS", [])
with open("indicador_inegi.csv", "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["periodo", "valor"])
    for obs in observaciones:
        w.writerow([obs["TIME_PERIOD"], obs["OBS_VALUE"]])
print(f"Guardados {len(observaciones)} registros")
#+END_SRC

** Resumen de enlaces

- INEGI API Indicadores: ~https://www.inegi.org.mx/servicios/api_indicadores.html~
- INEGI API DENUE: ~https://www.inegi.org.mx/servicios/api_denue.html~
- INEGI Datos Abiertos: ~https://www.inegi.org.mx/datosabiertos/~
- INEGI Descarga Masiva: ~https://www.inegi.org.mx/app/descarga/~
- Registro token INEGI: ~https://www.inegi.org.mx/app/desarrolladores/registro/default.html~
- datos.gob.mx: ~https://datos.gob.mx/~
- Constructor de Consultas INEGI: ~https://www.inegi.org.mx/app/querybuilder/~

** Dependencias

#+BEGIN_EXAMPLE
pip install requests inegipy
#+END_EXAMPLE
